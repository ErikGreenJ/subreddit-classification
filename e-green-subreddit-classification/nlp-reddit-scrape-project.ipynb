{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as datetime\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/search/submission/?subreddit=space&size=500&after=30d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note- this function was adapted from an in-class codealong taught by Brian Collins(TA, Washington DC, GA DSI-5)\n",
    "\n",
    "def query_pushshift(subreddit, kind='submission', skip=30, times=6, size=500, start=0, pseudoverbose=5,\n",
    "                   subfield = ['title', 'selftext', 'subreddit', 'created_utc',\n",
    "                               'author', 'num_comments', 'score', 'is_self'],\n",
    "                    comfields=['body', 'score', 'created_utc', 'subreddit', 'author']):\n",
    "#subreddit= name of the subreddit, \n",
    "#kind={'submission', 'comment'}, \n",
    "#skip=number of days to skip after each scrape,\n",
    "#times= how many iterations to do(one iter= 1 scrape, skip days once).\n",
    "#size = how many posts to scrape during each iteration.\n",
    "#start= how many days back to start from.  This parameter exists so you can pick up where you left off if you \n",
    "#didn't get as many posts as you would have liked.\n",
    "#pseudoverbose= prints updates after n cycles.  Default 5\n",
    "#subfield = all of the specific fields from each post we will be scraping and saving.  \n",
    "#other subfields can be found by visiting a subreddit on reddit.com, adding .json to the end of the url, and looking \n",
    "#through the 'data' dictionary keys\n",
    "#comfields = same as subfields, but for comments, should you choose to set the param kind='comment'\n",
    "    \n",
    "    \n",
    "    \n",
    "    stem = \"https://api.pushshift.io/reddit/search/{}/?subreddit={}&size={}\".format(kind, subreddit, size)\n",
    "    #stem is the url with certain fields missing.  The missing fields are filled using the .format method to pass \n",
    "    #parameters into a string\n",
    "    #also note that we are not scraping from reddit directly.  We are scraping from a datadase of reddit posts \n",
    "    #provided by pushshift.io, a website designed to help people gather data from social media and to serve as a data\n",
    "    #science teaching aid.  To donate to pushshift, visit https://pushshift.io/donations/.  \n",
    "    \n",
    "    mylist = []                                              #This empty list will become a list of dfs to be concatted\n",
    "                                                             #into a single df\n",
    "    \n",
    "    count = 0                                                #count keeps track of iters.  \n",
    "    for x in range(0, times):                                #loop will run for 'times=' iterations\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        URL = '{}&after={}d'.format(stem, skip * x + start)  #This sets up which URL will be scraped.\n",
    "        \n",
    "        response = requests.get(URL)                         #This where we save everything scraped \n",
    "        \n",
    "        assert response.status_code == 200                   #This only lets the loop continue as long as we've got \n",
    "                                                             #a status code of 200('all good')\n",
    "        \n",
    "        mine = response.json()['data']                       #This saves the data from json format as a variable.  \n",
    "                                                             #Data is a dictionary where all content is stored.\n",
    "        \n",
    "        df = pd.DataFrame.from_dict(mine)                    #this saves the dict as df, separated by dictionary key\n",
    "        \n",
    "        mylist.append(df)                                    #appends our list.  When complete, we'll have a list of\n",
    "                                                             #dataframes\n",
    "        \n",
    "        time.sleep(.25)                                      #time.sleep pauses our loop for n seconds.  I've set it so \n",
    "                                                             #low because we don't need to sleep when scraping from\n",
    "                                                             #pushshift\n",
    "        \n",
    "        if count%pseudoverbose == 0:                         #This was added to print how many times we've cycled\n",
    "            print(count, 'cycles complete.')                 #every n cycles.  It used to print the whole url, every\n",
    "                                                             #time, but that would fill the notebook with urls and \n",
    "                                                             #make scrolling through the notebook a bit busy for my tastes.\n",
    "        \n",
    "    full = pd.concat(mylist)                                 #full = a full df of all info scraped\n",
    "    mylist = []                                              #overwriting to dump out the list, for the sake of memory\n",
    "    \n",
    "    if kind == 'submission':                                 #the following is if kind is set to submission\n",
    "        \n",
    "        full = full[subfield]                                #this strips the df down to the fields we want\n",
    "        \n",
    "        full = full.drop_duplicates()                        #this drops duplicates\n",
    "        \n",
    "        full = full.loc[full['is_self'] == True]             #this drops rows where the main text is empty\n",
    "                    \n",
    "    elif kind == 'comment':                                  #the following is if kind is set to comment\n",
    "                    \n",
    "#         for thing in full['body']:                           #this makes it so we don't save empty rows\n",
    "#             if thing != '':\n",
    "                    \n",
    "        full = full[comfields]                               #strips the df to the fields we want\n",
    "\n",
    "        #full = full.drop_duplicates()                        #drops duplicates\n",
    "                \n",
    "    else:\n",
    "        print (\"kind must be 'submission' or 'comment'\")     #sends user a message if they haven't set the param correctly\n",
    "        \n",
    "        \n",
    "        \n",
    "#     def get_date(created):                                   #function that retrieves timestamp from when scrape occured\n",
    "#         return datetime.date.fromtimestamp(created)              \n",
    "    \n",
    "#     _timestamp = full['created_utc'].apply(get_date)         #creates a column of timestamps\n",
    "    \n",
    "#     full['timestamp'] = _timestamp                           #saves the timestamp to the df\n",
    "    \n",
    "    print(count, 'cycles complete.')\n",
    "    print(full.shape)\n",
    "    \n",
    "    \n",
    "    return full\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### interesting subreddits scraped for consideration included: <br>\n",
    "-showerthoughts  (13,636,977 subscribers, ranked \\# 24 on reddit)<br>\n",
    "-askscience (15,197,492 subscribers, ranked \\# 18 on reddit)<br>\n",
    "-democrat/republican <br>\n",
    "-space/sea \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decided to scrape from r/Democrats and r/Republicans, going back as many comments as possible going back to about early October 2016 (500 per day)\n",
    "### The body of most of the actual submissions were links or photos, so scraped comments as a better practice for NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that the number of rows is nearly maxed out (350000 would be max) for democrats most days, \n",
    "#there are more than 500 comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 cycles complete.\n",
      "100 cycles complete.\n",
      "150 cycles complete.\n",
      "200 cycles complete.\n",
      "250 cycles complete.\n",
      "300 cycles complete.\n",
      "350 cycles complete.\n",
      "400 cycles complete.\n",
      "450 cycles complete.\n",
      "500 cycles complete.\n",
      "550 cycles complete.\n",
      "600 cycles complete.\n",
      "650 cycles complete.\n",
      "700 cycles complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:64: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 cycles complete.\n",
      "(349271, 5)\n"
     ]
    }
   ],
   "source": [
    "dems_comment2 = query_pushshift('democrats', kind='comment', skip=1, times=700, size=500, pseudoverbose=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dems_comment2.to_csv('dems_raw_comment2.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 cycles complete.\n",
      "100 cycles complete.\n",
      "150 cycles complete.\n",
      "200 cycles complete.\n",
      "250 cycles complete.\n",
      "300 cycles complete.\n",
      "350 cycles complete.\n",
      "400 cycles complete.\n",
      "450 cycles complete.\n",
      "500 cycles complete.\n",
      "550 cycles complete.\n",
      "600 cycles complete.\n",
      "650 cycles complete.\n",
      "700 cycles complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:64: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 cycles complete.\n",
      "(315274, 5)\n"
     ]
    }
   ],
   "source": [
    "reps_comment2 = query_pushshift('republicans', kind='comment', skip=1, times=700, size=500, pseudoverbose=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps_comment2.to_csv('reps_raw_comment2.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basic observation #1:  dems more active on reddit.  Dem comments over max(by how much?, unknown), rep comments 90% of max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[deleted]             45662\n",
       "VegaThePunisher       25070\n",
       "AutoModerator          7360\n",
       "Gsteel11               3365\n",
       "backpackwayne          3157\n",
       "therecordcorrected     2918\n",
       "KubrickIsMyCopilot     2882\n",
       "data2dave              2241\n",
       "election_info_bot      2088\n",
       "Brysynner              1671\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dems_comment2.author.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FranklinAbernathy     17546\n",
       "[deleted]             14095\n",
       "Wannabe2good          13972\n",
       "AutoModerator          8995\n",
       "BobcatBarry            6314\n",
       "jesse11551             6153\n",
       "BaronBifford           5522\n",
       "chainsawx72            4213\n",
       "Cuckold-doodle-doo     3516\n",
       "BatMally               3346\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_comment2.author.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author value_counts suggest that author may be a good predictor, outside of NLP tasks.<br>\n",
    "<br>\n",
    "Also, notice the proportion of deleted to total in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "democrat deleted comment rate: 13.073515980427805\n",
      "republican deleted comment rate: 4.470714362744787\n"
     ]
    }
   ],
   "source": [
    "dems_deleted = (45662/349271)*100\n",
    "reps_deleted = (14095/315274)*100\n",
    "print(f'democrat deleted comment rate: {dems_deleted}')\n",
    "print(f'republican deleted comment rate: {reps_deleted}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Democrat comments are nearly 3 times as likely to be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "#https://stackoverflow.com/questions/45140034/python-convert-seconds-to-datetime-date-and-time\n",
    "#comment from 'Alex Hristov' taught me to convert datetime from seconds to utc time\n",
    "\n",
    "#following info from google/wikipedia:\n",
    "#Going to use UTC-4hrs to convert to eastern, note that this can be -5hrs too, depending on time of year.\n",
    "#Note that I could make this more precise, by subtracting another hour based on conditions that fit for \n",
    "#2x yearly time changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dems_comment2['created_utc'] = dems_comment2['created_utc'] - (60*60*4)  #subtracting 4 hrs from each utc in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps_comment2['created_utc'] = reps_comment2['created_utc'] - (60*60*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Friday, September 07, 2018 10:01:34'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.fromtimestamp(1536386494-14400).strftime(\"%A, %B %d, %Y %I:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not working, getting \"TypeError: cannot convert the series to <class 'int'>\"\n",
    "\n",
    "#dems_comment2['day_of_week'] = datetime.fromtimestamp(dems_comment2['created_utc']).strftime(\"%A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#breaking down timestamp for later analysis\n",
    "\n",
    "days = []\n",
    "for x in dems_comment2['created_utc']:\n",
    "    j = datetime.fromtimestamp(x).strftime(\"%A\")\n",
    "    days.append(j)\n",
    "\n",
    "months = []\n",
    "for x in dems_comment2['created_utc']:\n",
    "    j = datetime.fromtimestamp(x).strftime(\"%B\")\n",
    "    months.append(j)\n",
    "\n",
    "date = []\n",
    "for x in dems_comment2['created_utc']:\n",
    "    j = int(datetime.fromtimestamp(x).strftime(\"%d\"))\n",
    "    date.append(j)\n",
    "\n",
    "year = []\n",
    "for x in dems_comment2['created_utc']:\n",
    "    j = int(datetime.fromtimestamp(x).strftime(\"%y\"))\n",
    "    year.append(j)\n",
    "\n",
    "hour = []\n",
    "for x in dems_comment2['created_utc']:\n",
    "    j = int(datetime.fromtimestamp(x).strftime(\"%I\"))\n",
    "    hour.append(j)\n",
    "\n",
    "#note that I had no intention on breaking down to minute, but what if there is some weird connection\n",
    "#ex, comments spike during CNN/FOX news commercial breaks.  I don't intend on examining it, but the data could\n",
    "#have use\n",
    "\n",
    "minute = []\n",
    "for x in dems_comment2['created_utc']:\n",
    "    j = int(datetime.fromtimestamp(x).strftime(\"%M\"))\n",
    "    minute.append(j)\n",
    "\n",
    "dems_comment2['day_of_week'] = days\n",
    "dems_comment2['month'] = months\n",
    "dems_comment2['date'] = date\n",
    "dems_comment2['year'] = year\n",
    "dems_comment2['hour'] = hour\n",
    "dems_comment2['minute'] = minute\n",
    "\n",
    "#Rearranging columns\n",
    "dems_comment2 = pd.DataFrame(dems_comment2, columns = ['subreddit', 'author', 'score', 'created_utc', \n",
    "                         'day_of_week', 'month', 'date', 'year','hour', 'minute',\n",
    "                         'body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing for reps now\n",
    "\n",
    "days = []\n",
    "for x in reps_comment2['created_utc']:\n",
    "    j = datetime.fromtimestamp(x).strftime(\"%A\")\n",
    "    days.append(j)\n",
    "\n",
    "months = []\n",
    "for x in reps_comment2['created_utc']:\n",
    "    j = datetime.fromtimestamp(x).strftime(\"%B\")\n",
    "    months.append(j)\n",
    "\n",
    "date = []\n",
    "for x in reps_comment2['created_utc']:\n",
    "    j = int(datetime.fromtimestamp(x).strftime(\"%d\"))\n",
    "    date.append(j)\n",
    "\n",
    "year = []\n",
    "for x in reps_comment2['created_utc']:\n",
    "    j = int(datetime.fromtimestamp(x).strftime(\"%y\"))\n",
    "    year.append(j)\n",
    "\n",
    "hour = []\n",
    "for x in reps_comment2['created_utc']:\n",
    "    j = int(datetime.fromtimestamp(x).strftime(\"%I\"))\n",
    "    hour.append(j)\n",
    "\n",
    "#note that I had no intention on breaking down to minute, but what if there is some weird connection\n",
    "#ex, comments spike during CNN/FOX news commercial breaks.  I don't intend on examining it, but the data could\n",
    "#have use\n",
    "\n",
    "minute = []\n",
    "for x in reps_comment2['created_utc']:\n",
    "    j = int(datetime.fromtimestamp(x).strftime(\"%M\"))\n",
    "    minute.append(j)\n",
    "\n",
    "reps_comment2['day_of_week'] = days\n",
    "reps_comment2['month'] = months\n",
    "reps_comment2['date'] = date\n",
    "reps_comment2['year'] = year\n",
    "reps_comment2['hour'] = hour\n",
    "reps_comment2['minute'] = minute\n",
    "\n",
    "#Rearranging columns\n",
    "reps_comment2 = pd.DataFrame(reps_comment2, columns = ['subreddit', 'author', 'score', 'created_utc', \n",
    "                         'day_of_week', 'month', 'date', 'year','hour', 'minute',\n",
    "                         'body'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating subreddits to switch subreddits to targets 1 and 0\n",
    "\n",
    "dems_comment2['subreddit'] = 1\n",
    "\n",
    "reps_comment2['subreddit'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>VegaThePunisher</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.536372e+09</td>\n",
       "      <td>Friday</td>\n",
       "      <td>September</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>And told us to vote.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>IAMA_Drunk_Armadillo</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.536373e+09</td>\n",
       "      <td>Friday</td>\n",
       "      <td>September</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Aww did the Nazi snowflake get triggered? You ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>IAMA_Drunk_Armadillo</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.536373e+09</td>\n",
       "      <td>Friday</td>\n",
       "      <td>September</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Aww did the Nazi snowflake get triggered?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>FyreTroll</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.536374e+09</td>\n",
       "      <td>Friday</td>\n",
       "      <td>September</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Excuse me, what?\\nThat logic also applies to “...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>FyreTroll</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.536374e+09</td>\n",
       "      <td>Friday</td>\n",
       "      <td>September</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>Your comment made no actual factual refutation...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                author  score   created_utc day_of_week  \\\n",
       "0        1.0       VegaThePunisher    1.0  1.536372e+09      Friday   \n",
       "1        1.0  IAMA_Drunk_Armadillo    1.0  1.536373e+09      Friday   \n",
       "2        1.0  IAMA_Drunk_Armadillo    1.0  1.536373e+09      Friday   \n",
       "3        1.0             FyreTroll    1.0  1.536374e+09      Friday   \n",
       "4        1.0             FyreTroll    1.0  1.536374e+09      Friday   \n",
       "\n",
       "       month  date  year  hour  minute  \\\n",
       "0  September   7.0  18.0  10.0     1.0   \n",
       "1  September   7.0  18.0  10.0     9.0   \n",
       "2  September   7.0  18.0  10.0    11.0   \n",
       "3  September   7.0  18.0  10.0    31.0   \n",
       "4  September   7.0  18.0  10.0    34.0   \n",
       "\n",
       "                                                body  \n",
       "0                              And told us to vote.   \n",
       "1  Aww did the Nazi snowflake get triggered? You ...  \n",
       "2         Aww did the Nazi snowflake get triggered?   \n",
       "3  Excuse me, what?\\nThat logic also applies to “...  \n",
       "4  Your comment made no actual factual refutation...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#continually loading from kernel crashes\n",
    "\n",
    "#reps_comment2.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "#dems_comment2.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "dems_comment2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"latenight = 0\\nmorning = 0\\nafternoon = 0\\nevening = 0\\n\\nfor x in dems_comment2.hour:\\n    if x in [22, 23, 0, 1, 2, 3]:\\n        latenight += 1\\n    elif x in [4, 5,6,7,8,9]:\\n        morning += 1\\n    elif x in [10,11,12,13,14,15]:\\n        afternoon += 1\\n    elif x in [16,17,18,19,20,21]:\\n        evening += 1\\nprint(latenight/dems_comment2.shape[0], 'democrats latenight percentage')\\nprint(morning/dems_comment2.shape[0], 'democrats morning percentage')\\nprint(afternoon/dems_comment2.shape[0], 'democrats afternoon percentage')\\nprint(evening/dems_comment2.shape[0], 'democrats evening percentage')\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#would work if used AM/PM\n",
    "\n",
    "'''latenight = 0\n",
    "morning = 0\n",
    "afternoon = 0\n",
    "evening = 0\n",
    "\n",
    "for x in dems_comment2.hour:\n",
    "    if x in [22, 23, 0, 1, 2, 3]:\n",
    "        latenight += 1\n",
    "    elif x in [4, 5,6,7,8,9]:\n",
    "        morning += 1\n",
    "    elif x in [10,11,12,13,14,15]:\n",
    "        afternoon += 1\n",
    "    elif x in [16,17,18,19,20,21]:\n",
    "        evening += 1\n",
    "print(latenight/dems_comment2.shape[0], 'democrats latenight percentage')\n",
    "print(morning/dems_comment2.shape[0], 'democrats morning percentage')\n",
    "print(afternoon/dems_comment2.shape[0], 'democrats afternoon percentage')\n",
    "print(evening/dems_comment2.shape[0], 'democrats evening percentage')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now will merge randomly.\n",
    "df = pd.concat([dems_comment2, reps_comment2], ignore_index=True)\n",
    "\n",
    "#learned about 'ignore_index=' from pandas.concat documentation @\n",
    "#https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True) #this will shuffle all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>waldrop02</td>\n",
       "      <td>11</td>\n",
       "      <td>1490791891</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>March</td>\n",
       "      <td>29</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>51</td>\n",
       "      <td>Him being a rude person in general doesn't eli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>nakdamink</td>\n",
       "      <td>1</td>\n",
       "      <td>1508882556</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>October</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>I fully agree, Moore holds absolutely reprehen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>HankMoodyMF</td>\n",
       "      <td>1</td>\n",
       "      <td>1511794509</td>\n",
       "      <td>Monday</td>\n",
       "      <td>November</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>55</td>\n",
       "      <td>These guys are awful for the left.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>mydadisnotyourdad</td>\n",
       "      <td>1</td>\n",
       "      <td>1507227093</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>October</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Not sure where the \"slam\" was.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>BobcatBarry</td>\n",
       "      <td>9</td>\n",
       "      <td>1499876621</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>July</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>I think \"destroys\" is a pretty strong word for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Credulous7</td>\n",
       "      <td>1</td>\n",
       "      <td>1495068773</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>May</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>52</td>\n",
       "      <td>Written evidence of collaboration between Russ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Pylons</td>\n",
       "      <td>1</td>\n",
       "      <td>1535807729</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>September</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>Yes, I am aware. That's not what I'm saying. \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>orr250mph</td>\n",
       "      <td>7</td>\n",
       "      <td>1481736275</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>December</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>Bush lied and people died.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1</td>\n",
       "      <td>1499656564</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>July</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>deleted  ^^^^^^^^^^^^^^^^0.1027 .message here....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>crosscheck87</td>\n",
       "      <td>1</td>\n",
       "      <td>1531091225</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>July</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>I'll take a link to where he said that, you ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit             author  score  created_utc day_of_week      month  \\\n",
       "0          1          waldrop02     11   1490791891   Wednesday      March   \n",
       "1          0          nakdamink      1   1508882556     Tuesday    October   \n",
       "2          1        HankMoodyMF      1   1511794509      Monday   November   \n",
       "3          0  mydadisnotyourdad      1   1507227093    Thursday    October   \n",
       "4          0        BobcatBarry      9   1499876621   Wednesday       July   \n",
       "5          1         Credulous7      1   1495068773   Wednesday        May   \n",
       "6          1             Pylons      1   1535807729    Saturday  September   \n",
       "7          1          orr250mph      7   1481736275   Wednesday   December   \n",
       "8          1          [deleted]      1   1499656564      Sunday       July   \n",
       "9          1       crosscheck87      1   1531091225      Sunday       July   \n",
       "\n",
       "   date  year  hour  minute                                               body  \n",
       "0    29    17     8      51  Him being a rude person in general doesn't eli...  \n",
       "1    24    17     6       2  I fully agree, Moore holds absolutely reprehen...  \n",
       "2    27    17     9      55                These guys are awful for the left.   \n",
       "3     5    17     2      11                    Not sure where the \"slam\" was.   \n",
       "4    12    17    12      23  I think \"destroys\" is a pretty strong word for...  \n",
       "5    17    17     8      52  Written evidence of collaboration between Russ...  \n",
       "6     1    18     9      15  Yes, I am aware. That's not what I'm saying. \\...  \n",
       "7    14    16    12      24                         Bush lied and people died.  \n",
       "8     9    17    11      16  deleted  ^^^^^^^^^^^^^^^^0.1027 .message here....  \n",
       "9     8    18     7       7  I'll take a link to where he said that, you ca...  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping deleted and removed posts, 59757 in total\n",
    "\n",
    "df.drop(df[df['author'] == '[deleted]'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.502009\n",
       "0    0.497991\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#after dropping deleted comments, classes are balanced nearly perfectly\n",
    "df.subreddit.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(604788, 11)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dems_comment2.to_csv('dems_raw_comment2.csv') \n",
    "reps_comment2.to_csv('reps_raw_comment2.csv') \n",
    "df.to_csv('df_dem_rep_comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup \n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Cleaning function from class to remove any HTML, all non-letters, stopwords, and making all words lowercase.  Also, added a step to Lemmatize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>waldrop02</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.490792e+09</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>March</td>\n",
       "      <td>29.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>Him being a rude person in general doesn't eli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nakdamink</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.508883e+09</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>October</td>\n",
       "      <td>24.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I fully agree, Moore holds absolutely reprehen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HankMoodyMF</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.511795e+09</td>\n",
       "      <td>Monday</td>\n",
       "      <td>November</td>\n",
       "      <td>27.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>These guys are awful for the left.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mydadisnotyourdad</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.507227e+09</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>October</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Not sure where the \"slam\" was.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>BobcatBarry</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.499877e+09</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>July</td>\n",
       "      <td>12.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>I think \"destroys\" is a pretty strong word for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0  subreddit             author  score   created_utc day_of_week  \\\n",
       "0          0        1.0          waldrop02   11.0  1.490792e+09   Wednesday   \n",
       "1          1        0.0          nakdamink    1.0  1.508883e+09     Tuesday   \n",
       "2          2        1.0        HankMoodyMF    1.0  1.511795e+09      Monday   \n",
       "3          3        0.0  mydadisnotyourdad    1.0  1.507227e+09    Thursday   \n",
       "4          4        0.0        BobcatBarry    9.0  1.499877e+09   Wednesday   \n",
       "\n",
       "      month  date  year  hour  minute  \\\n",
       "0     March  29.0  17.0   8.0    51.0   \n",
       "1   October  24.0  17.0   6.0     2.0   \n",
       "2  November  27.0  17.0   9.0    55.0   \n",
       "3   October   5.0  17.0   2.0    11.0   \n",
       "4      July  12.0  17.0  12.0    23.0   \n",
       "\n",
       "                                                body  \n",
       "0  Him being a rude person in general doesn't eli...  \n",
       "1  I fully agree, Moore holds absolutely reprehen...  \n",
       "2                These guys are awful for the left.   \n",
       "3                    Not sure where the \"slam\" was.   \n",
       "4  I think \"destroys\" is a pretty strong word for...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.body()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapted from code in GA lesson nlp-i-notes-starter-code\n",
    "#Step Four: Combine our cleaning into one function\n",
    "#Lesson given by Matt Brems.\n",
    "#added step to lemmatize.\n",
    "\n",
    "def review_to_words(raw_review):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(str(raw_review)).get_text()          \n",
    "                                     #No clue why, but this was working and now not working after dead kernel\n",
    "        \n",
    "                                    #somehow, somewhere, something turned into a float.  \n",
    "                                    #so by wrapping raw_review with str() in 2 steps, problem seems fixed\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", str(raw_review))\n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()\n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words('english'))\n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 6. Lemmatize words  not in Matt's original function\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_words = [lemmatizer.lemmatize(w) for w in meaningful_words]\n",
    "    \n",
    "    # 7. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return (' '.join(lem_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Users/erikgreenj/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://media2.giphy.com/media/3oriO8jjWHjsEK4kms/giphy.gif\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.tallahassee.com/story/news/2018/02/15/florida-school-shooting-suspect-nikolas-cruz-member-white-nationalist-militia-tallahassee-leader-say/341751002/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://m.imgur.com/FHX5Jyv\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=BOlwvuiNy6g\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.cnn.com/2018/06/08/us/teen-killed-after-ice-returns-him-to-mexico/index.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://img00.deviantart.net/16ad/i/2016/319/0/2/2016__election_results_by_county_by_ynot1989-daoioo0.png\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://electionlandtrends.appspot.com\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://blog.smu.edu/research/2017/11/14/study-settles-prehistoric-puzzle-finds-carbon-dioxide-link-global-warming-22-million-years-ago/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.cnn.com/2017/11/20/politics/al-franken-inappropriate-touch-2010/index.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.google.com/amp/s/www.washingtonpost.com/amphtml/news/morning-mix/wp/2017/08/28/black-clad-antifa-attack-right-wing-demonstrators-in-berkeley/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://ballotpedia.org/ACORN_and_voter_registration_fraud\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://m.youtube.com/watch?v=TCm9788Tb5g\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.google.com/amp/www.newsweek.com/could-president-trump-repeal-obamacare-518992%3famp=1\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.marketwatch.com/story/gun-maker-stocks-surge-after-mass-shooting-in-las-vegas-2017-10-02\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://projects.fivethirtyeight.com/trump-approval-ratings/?ex_cid=rrpromo\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://i.redd.it/z562knddi2g11.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.thefiscaltimes.com/2014/10/17/Who-s-More-Generous-Liberals-or-Conservatives%3Famp&amp;ved=2ahUKEwioo8rkhanZAhWLT98KHRIiDegQFjAAegQIDxAB&amp;usg=AOvVaw3gEma4Ns5svS8dpG8etC58&amp;ampcf=1\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://faculty.tamucc.edu/sfriday/wordpress/?p=2875\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://m.youtube.com/watch?v=OM5kT_p4-K8\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://m.youtube.com/watch?v=g67yxre_8OY&amp;ebc=ANyPxKrQ7CCPoPnUkZs0cT-L0RPhZn-Cmguz1Tw89ibi3F4rwBevkREgnq2Xt2-KH5VCIKFdQ9YkET9c8C_LmTCM2i5wWMLkRg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.google.com/amp/s/patch.com/minnesota/southwestminneapolis/amp/27609020/mn-economy-tops-wisconsins-virtually-every-measure-report\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.pastemagazine.com/articles/2016/11/tulsi-gabbard-is-not-who-you-think-she-is.html.\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.google.com/amp/s/thinkprogress.org/amp/p/4e1c391b63f5\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.crowdpac.com/campaigns/387413/either-sen-collins-votes-no-on-kavanaugh-or-we-fund-her-future-opponent\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.theonion.com/no-way-to-prevent-this-says-only-nation-where-this-r-1819576527\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://m.democracynow.org/stories/16948\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.latimes.com/nation/nationnow/la-na-pol-trump-mexico-call-20170201-story.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://i.imgur.com/GX2YiHk.gif\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://pbs.twimg.com/profile_images/2708901225/96c3ddf1dc4c0e84a34626de22096a4d.jpeg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.wikipedia.org/wiki/Why_Not_Me%3F_(novel)\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.vox.com/policy-and-politics/2017/1/20/14332462/photos-crowd-trump-inauguration-vs-obama\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.uwgb.edu/dutchs/PSEUDOSC/Rep-DemSwitch.HTM\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://g.redditmedia.com/AsHUSdo7N8ASf8d4LCFOTrMzFZJ31D0Mjm3xtGHx-CA.gif?fm=mp4&amp;mp4-fragmented=false&amp;s=074c36e2c9c4a2fc150ae5318c9123dd\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://imgur.com/a/iDiLe\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.google.be/url?sa=t&amp;source=web&amp;rct=j&amp;url=http://www.snopes.com/hillary-clinton-uranium-russia-deal/&amp;ved=0ahUKEwidj6n05IPTAhWhCcAKHf0bC1AQFggaMAA&amp;usg=AFQjCNH783yopFvmerW5Rpy8rCv6PaHQQg&amp;sig2=57qOVVwdnN5uXIFhRwrU0g\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/pXyryUHItuc\n",
      "\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.scribd.com/document/109398239/2008-2012-Elections-Results-Anomalies-and-Analysis\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://i.imgur.com/seh6p.gif\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.wikipedia.org/wiki/Negative_and_positive_rights\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.mediaite.com/online/trumps-lonely-photo-op-targeting-pelosi-and-schumer-inspires-twitter-memes/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://static.fjcdn.com/pictures/Trump_f150a2_2043423.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=cruh2p_Wh_4\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.huffingtonpost.com/entry/donald-trump-racist-examples_us_56d47177e4b03260bf777e83\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/FkMWsO_-X7Y\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/ycZj5-5KJwA\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://abcnews.go.com/Politics/president-trump-renews-government-shutdown-threat-border-wall/story?id=56914077\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.military.com/daily-news/2018/03/15/pentagon-wants-more-funding-boost-training-after-deadly-crash.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.wikipedia.org/wiki/Southern_strategy\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.mcclatchydc.com/news/politics-government/white-house/article136939868.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.google.com/amp/www.pewresearch.org/fact-tank/2016/11/09/why-2016-election-polls-missed-their-mark/%3famp=1\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.snopes.com/2017/03/30/evelyn-farkas-leak/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.wikipedia.org/wiki/Wilbur_Ross\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=hkx4gGfmUVs\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.indivisibleguide.com/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://siarchives.si.edu/history/smithsonian-institution-building-castle\n",
      "\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/6imFvSua3Kg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://i.imgur.com/TMmlKMJ.gif\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.espn.com/nfl/story/_/id/18964343/donald-trump-takes-shot-colin-kaepernick-free-agent-status\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://images.encyclopediadramatica.rs/f/fd/Picard-no-facepalm.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.wikipedia.org/wiki/Medical_tourism\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.m.wikipedia.org/wiki/Argument\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://twitter.com/markfollman/status/862156711380922368\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://days.to/election-day-in-us/2016\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://littlegreenfootballs.com/article/47449_On_the_Same_Day_Trump_Disavows_White_Supremacists_Then_Retweets_a_White_Supremacist#rss\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=DjW0dk2_fgk\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.amnestyusa.org/issues/death-penalty/death-penalty-facts/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.washingtonpost.com/opinions/obama-can-appoint-merrick-garland-to-the-supreme-court-if-the-senate-does-nothing/2016/04/08/4a696700-fcf1-11e5-886f-a037dba38301_story.html?utm_term=.f86401e6e33f\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://2.bp.blogspot.com/-dMM2J1wNoNU/VO9WmBTA1yI/AAAAAAAAJZE/EF0Y_QMKOWg/s1600/it-is-time.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://orwell.ru/library/essays/nationalism/english/e_nat\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.reddit.com/r/republicans/comments/7e2qwq/magic_wand_trump_brings_jobs_back_to_america/dq3liiy/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://imgur.com/9nqNwXW\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.m.wikipedia.org/wiki/Ageism\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://explorepahistory.com/kora/files/1/2/1-2-1934-25-ExplorePAHistory-a0m0a2-a_349.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://i3.kym-cdn.com/photos/images/facebook/000/865/302/3d5.gif\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://m.facebook.com/groups/1101122606660880?ref=bookmarks\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.washingtonpost.com/news/the-fix/wp/2016/04/02/in-the-year-of-trump-a-made-up-news-website-run-by-an-ex-convict-finds-success/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=qvJeATp31dw\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.npr.org/2017/11/03/561976645/clinton-campaign-had-additional-signed-agreement-with-dnc-in-2015\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://forward.com/news/national/371019/meet-hannah-risheq-americas-first-jewish-muslim-politician/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.wikipedia.org/wiki/History_of_the_United_States_Democratic_Party\n",
      "\n",
      "https://medium.com/everyvote/how-the-republicans-and-democrats-switched-on-civil-rights-in-5-racist-steps-92c1b41480b\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://i.imgur.com/Jpi9CvA.mp4\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=eXWhbUUE4ko\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.local10.com/news/crime/man-forces-victim-into-miami-shores-home-before-forcing-him-to-drive-to-atm-police-say\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.dhs.gov/news/2017/09/25/federal-government-continues-hurricane-maria-response-and-relief-operations\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.thenation.com/article/july-13-1960-john-f-kennedy-secures-the-democratic-presidential-nomination/\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.cbsnews.com/news/syria-chemical-attack-trump-threatens-bashar-animal-assad-putin-chemical-weapons-strike-2018-04-08/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://imgur.com/gallery/jS0sc\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://i.imgflip.com/1ajp9f.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.wikipedia.org/wiki/Ferguson_unrest\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.foxnews.com/entertainment/2017/06/02/kathy-griffins-photo-shoot-7-celebrities-standing-by-her.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.wikipedia.org/wiki/Pennsylvania%27s_7th_congressional_district?wprov=sfla1\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.reddit.com/r/democrats/comments/6majj5/imagine_president_trump_at_ground_zero/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.washingtonpost.com/news/monkey-cage/wp/2017/08/24/did-enough-bernie-sanders-supporters-vote-for-trump-to-cost-clinton-the-election/?utm_term=.c6eed5a8f825\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.thoughtsandprayersthegame.com/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.christineforvermont.com/issues/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://en.wikipedia.org/wiki/False_dilemma\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.nbclosangeles.com/news/local/San-Pedro-Man-Plead-Guilty-Threatening-Kill-Rep-Maxine-Waters-479869893.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://newsfeed.time.com/2012/11/15/research-suggests-humans-are-evolving-to-be-dumber/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=0RBL54ANyao\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.m.wikipedia.org/wiki/Social_dominance_orientation\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=AjQxI9CtzbI\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.wikipedia.org/wiki/Single-payer_healthcare\n",
      "\n",
      "https://www.vox.com/cards/single-payer/which-countries-have-single-payer-health-care-systems\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/d-diB65scQU?t=24\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://i.imgur.com/zUmVQXS.png\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/4UDILid-Cu8\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://nymag.com/daily/intelligencer/2017\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://crayfisher.files.wordpress.com/2014/05/benghazi_relationships.png\n",
      "\n",
      "https://thefederalistpapers.org/wp-content/uploads/2017/03/imageedit_645_3117981996.jpg\n",
      "\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://mobile.twitter.com/JoyAnnReid/status/904522127172358144\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://getyarn.io/yarn-clip/3e5a5048-d765-4d0f-8551-af2b8a191916\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://s3.amazonaws.com/gs-geo-images/03978816-6802-430c-9c74-b88d419bb062_l.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.washingtonpost.com/news/the-fix/wp/2017/03/31/the-gap-between-republicans-and-democrats-views-of-african-americans-just-hit-a-new-high/?utm_term=.17957579cf59\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.elections.ny.gov/RunningOffice.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://i0.kym-cdn.com/entries/icons/mobile/000/023/397/C-658VsXoAo3ovC.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=giuZdBAXVh0\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://i.imgur.com/ynSFl5F.png\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://np.reddit.com/r/democrats/comments/8l1uro/bernies_army_in_disarray/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=6DGNZnfKYnU\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.nationalreview.com/corner/438059/how-white-house-correspondents-dinner-gave-us-trump\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://i.imgur.com/GxKsxdz.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.advocate.com/politics/2016/11/29/24-trump-fighting-charities-need-your-dollars\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://fortune.com/2017/07/16/amazon-postal-service-subsidy/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.nytimes.com/2018/02/15/us/politics/trumps-inaugural-committee-paid-26-million-to-first-ladys-friend.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://i.imgur.com/lDBuybD.png\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.snopes.com/fact-check/does-law-family-separation-detention-minors/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/MJzdgZ1lOTA\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.bizjournals.com/phoenix/news/2018/01/22/top-1-percent-rake-in-82-of-economic-stock-gains.html\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://andrewgillum.com/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/Gc-LJ_3VbUA\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.google.com/search?q=neo+nazi+republican&amp;oq=neo+nazi+republican&amp;aqs=chrome..69i57j69i61.2683j0j7&amp;sourceid=chrome&amp;ie=UTF-8&amp;safe=high\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.latimes.com/politics/washington/la-na-essential-washington-updates-trump-finally-comments-on-russia-s-1502398622-htmlstory.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.democrats.org/party-platform\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.aclu.org\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://mobile.nytimes.com/2018/06/01/business/economy/jobs-recovery-longer-view.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://i.kym-cdn.com/entries/icons/original/000/010/692/19789999.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://thumbs.dreamstime.com/b/silhouette-man-praying-top-against-beautiful-cloudy-sky-60374180.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://imgur.com/izbpudT.png\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.reddit.com/r/democrats/comments/8ywdv9/comment/e2eg8p4?st=JJM215OB&amp;sh=3d0c98d3\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.washingtonpost.com/posteverything/wp/2016/12/01/bernie-sanders-carrier-just-showed-corporations-how-to-beat-donald-trump/?utm_term=.1805ece9f53c\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://lawyerscommittee.org/press-release/lawyers-committee-civil-rights-law-files-lawsuit-challenging-georgias-voter-registration-scheme-2/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://discord.gg/B8M6UaC\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://boingboing.net/2018/06/18/fox-news-host-says-kids-are-be.html/amp\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.amazon.com/Bailout-Nation-Barry-Ritholtz/dp/1522689281\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.google.com/amp/s/www.haaretz.com/amp/us-news/d-souza-recently-pardoned-by-trump-shares-tweet-with-burnthejews-1.6224329\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.cnbc.com/2018/02/15/trump-inaugural-committee-paid-26-million-to-friend-of-melania-trump.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.usconstitution.net/constam.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://i.imgur.com/6gf8vuB.png\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://i.imgur.com/a/1hOeF.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.google.com/amp/s/www.vox.com/platform/amp/policy-and-politics/2018/2/1/16956290/nunes-memo-release-the-memo-fbi-russia\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.theroot.com/in-event-honoring-mlk-bernie-sanders-comments-on-race-1825043927\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://imgur.com/3nbyu5v\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.nytimes.com/2017/07/21/opinion/if-trump-pardons-crime-russia.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.deathandtaxesmag.com/317331/rep-john-lewis-endorses-keith-ellison-for-dnc-chair/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://i.pinimg.com/736x/1b/99/62/1b99620156224e7d50f8680f619629e4--travis-alexander-jodi-arias.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://thecitizenscorner.files.wordpress.com/2016/11/image14.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=7xxgRUyzgs0\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://np.reddit.com/r/EnoughTrumpSpam/comments/5aycqc/im_getting_real_sick_of_the_government_wasting_my/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.washingtonpost.com/video/politics/trump-on-ms-13-gang-members-theyre-animals/2018/05/17/830b2362-5a0c-11e8-9889-07bcc1327f4b_video.html?utm_term=.20a999ab2fde\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.comicbookfx.com/images/54-5.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://imgur.com/a/niXPIuk\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/dBkwlclpxjo\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.tedcruzforhumanpresident.com/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://i2.kym-cdn.com/entries/icons/original/000/022/524/tumblr_o16n2kBlpX1ta3qyvo1_1280.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/Rt-ldMj9y9w\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.theguardian.com/us-news/2017/may/11/donald-trump-james-comey-firing-russia-investigation\n",
      "\n",
      "http://www.businessinsider.com/president-trump-asked-comey-end-investigation-michael-flynn-before-firing-2017-5\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=KG9GFzM7zc4\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://i.sli.mg/HDRMLJ.png\n",
      "\n",
      "LOL\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://i.ytimg.com/vi/WGSv5GP73-4/maxresdefault.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://i2.kym-cdn.com/photos/images/newsfeed/000/283/235/7e3.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://orig00.deviantart.net/2c78/f/2012/126/5/c/calvin_and_hobbes_laughing_by_danidarko96-d4yrwih.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://i.imgur.com/mXyupD1.gif\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://mobile.twitter.com/factcheckdotorg/status/1031700515556605952\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://c2.staticflickr.com/8/7558/15792939698_a937c39c30_b.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://i.imgur.com/kx2ZQnH.png\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/ZYXygIcIJ6I\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.wikipedia.org/wiki/George_W._Bush\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://np.reddit.com/r/WayOfTheBern/comments/8l0ej6/bernies_army_in_disarray/dzbwciv/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.bostonglobe.com/opinion/2015/10/15/bernie-sanders-scandinavia-not-socialist-utopia/lUk9N7dZotJRbvn8PosoIN/story.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=FQLOeJXJHyU\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://twitter.com/instantsunrise/status/1033959100327976960\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://petitions.whitehouse.gov/petition/we-people-call-resignation-fcc-chairman-ajit-varadaraj-pai\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/mNeqrTbkZmM\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.politico.com/interactives/2018/02/01/what-is-in-the-nunes-memo-fbi-released-analysis/\n",
      "\n",
      "https://www.washingtonpost.com/blogs/right-turn/wp/2018/02/02/nunes-drops-his-cherry-picked-memo-this-is-it/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.commondreams.org/views/2017/07/24/wapo-worships-principled-humanitarian-mccain-thats-never-existed\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.slate.com/articles/news_and_politics/politics/2017/12/the_republicans_have_built_an_uneven_playing_field_of_morality.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.google.com/search?q=affordable+housing+crisis&amp;oq=affordable+housing+crisis&amp;aqs=chrome..69i57j69i60j0j69i60j0l2.2206j0j7&amp;sourceid=chrome&amp;ie=UTF-8&amp;safe=high\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.washingtonpost.com/news/morning-mix/wp/2015/05/16/congressman-who-advised-ex-wife-to-seek-abortion-votes-for-late-term-abortion-ban/?utm_term=.1133b8300d3e\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.washingtonpost.com/politics/how-the-nra-helped-put-bernie-sanders-in-congress/2015/07/19/ed1be26c-2bfe-11e5-bd33-395c05608059_story.html?utm_term=.b79ac7b0e24a\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.goskagit.com/news/man-pleads-not-guilty-in-father-s-stabbing-death/article_479b3b6f-88d4-502d-ae77-ff5f098fb511.html\n",
      "\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.christineforvermont.com/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://occupydemocrats.com/2016/11/10/putin-admits-coordinating-trump-campaign-election/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.wikipedia.org/wiki/Bulverism\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://static.highsnobiety.com/wp-content/uploads/2012/10/nike-just-do-it-wieden-kennedy-1-480x343.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.washingtonpost.com/news/post-politics/wp/2017/08/10/trump-says-he-is-very-thankful-to-putin-for-expelling-u-s-diplomats-from-russia/\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://us-east-1.tchyn.io/snopes-production/uploads/2018/07/trump_mouth_musical_quote_meme.jpg?resize=865%2C452\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://bootheglobalperspectives.com/articles/images/1397908221WBG243images.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.google.com/search?q=afmilies+killed+by+drunk+drivers&amp;oq=afmilies+killed+by+drunk+drivers&amp;aqs=chrome..69i57j0.6605j0j8&amp;sourceid=chrome&amp;ie=UTF-8\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.latimes.com/books/la-et-jc-james-comey-book-tour-20180402-story.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.google.com/amp/thehill.com/blogs/blog-briefing-room/news/326820-sanders-defends-trump-voters-i-dont-think-theyre-racists%3famp\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.vox.com/policy-and-politics/2017/3/8/14848636/hillary-clinton-tv-ads\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.wikipedia.org/wiki/Kent_State_shootings\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://cdn.barstoolsports.net/wp-content/uploads/2016/09/30/6-sunderland.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.reddit.com/r/HomeworkHelp/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.businessinsider.com/ocasio-cortez-gets-the-economics-of-budget-deficits-job-creation-2018-7?utm_source=reddit.com\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://i.pinimg.com/originals/a9/58/dc/a958dc93e49f72fb444abd247caa1810.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.nytimes.com/interactive/2017/06/23/opinion/trumps-lies.html?mcubz=0\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://southpark.cc.com/clips/165714/whats-the-score-jefe\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.salon.com/2018/06/30/exclusive-accused-annapolis-shooter-had-deep-dark-links-to-the-alt-right/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://worldmediamonitoring.com/list-quran-hate-quotes/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://livingnewdeal.org/what-was-the-new-deal/programs/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://np.reddit.com/r/BlueMidterm2018/comments/63phbt/okay_guys_im_going_to_level_with_you_the_election/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://media2.giphy.com/media/VzbN9gupkkXp6/giphy.gif\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://i.imgur.com/jyqERd3.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://i.redd.it/sa118g25oq201.png\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.snopes.com/fact-check/is-epa-allowing-asbestos-products/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/WIARMMtUdZQ\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.wikipedia.org/wiki/Dog-whistle_politics\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://tedcruzforhumanpresident.com\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://np.reddit.com/r/Liberal/comments/7btwrv/why_are_conservatives_more_susceptible_to/dpkr8ar/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://twitter.com/igorvolsky/status/798204162873769984\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.newsweek.com/anti-trump-protesters-take-knee-atlanta-football-game-president-has-vip-seats-773764\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=kdqyG3CcoLM\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.huffingtonpost.com/entry/democrats-letter-chaffetz-trump-conflicts-of-interest_us_583ca265e4b04b66c01b6c5a\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/1TphEh0Qgv0\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://imgur.com/nOBiNxj\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://s3.amazonaws.com/uploads.democrats.org/Downloads/DNC_Charter__Bylaws_9.17.15.pdf\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT7PR7rsB0vfkqD6X5vZRYMertdd68wResiVkoBJnQu7du6hqDZKQ\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://mobile.reuters.com/article/amp/idUSKCN1IW1NX\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://insights.som.yale.edu/insights/does-immigration-create-jobs\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.snopes.com/democrat-shooters-list/\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.niehs.nih.gov/health/topics/agents/air-pollution/index.cfm\n",
      "http://www.who.int/mediacentre/factsheets/fs313/en/\n",
      "https://www.epa.gov/haps/health-and-environmental-effects-hazardous-air-pollutants\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/Vcf6qE_b2_w\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.vox.com/2017/7/25/16030616/mccain-senate-health-care-bill\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://jacobinmag.com/2018/07/medicare-for-all-savings-mercatus-center-report\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.chicagotribune.com/news/opinion/chapman/ct-donald-trump-incompetent-president-chapman-perspec-20201010-column.html\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.vanityfair.com/news/2018/04/trump-war-with-amazon-and-the-washington-post-is-personal\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.childtrends.org/wp-content/uploads/2018/02/gun-violence-chart-update.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAAkPAAAAJGMxNzQzZDQ3LTQyYWYtNDRiYy04YjM4LTZkNDZlZDcwZjAwOQ.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://i.redd.it/p01p1hxcuvr01.png\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.npr.org/2016/08/17/490102648/the-clintons-wrote-the-book-on-how-politicians-climb-out-of-middle-class\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://mobile.twitter.com/emma4change/status/988036115579002881?s=21\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://dogearedpreacher.files.wordpress.com/2010/07/garylarson_fence2.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://twitter.com/McFaul/status/823582789039624192\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://i.imgur.com/3db8wHn.gif\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/4Pjs7uoOkag\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.google.com/amp/s/mobile.nytimes.com/2018/03/04/us/melania-trump-einstein-visa.amp.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://abdulformichigan.com/water\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.theatlantic.com/magazine/archive/1973/07/the-last-days-of-the-president/376281/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.nytimes.com/2017/05/08/us/politics/kushner-china-visa-eb-5.html?_r=0\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://i.imgur.com/B3NLmUq.png\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.snopes.com/fact-check/john-mccain-war-crimes/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=2alSjQUejzk\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://news.gallup.com/poll/207614/sharply-fewer-democrats-say-proud-americans.aspx\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://twitter.com/dhookstead/status/997529596600967168\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://pics.me.me/why-waste-time-say-lot-word-when-few-word-do-28283649.png\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.globalresearch.ca/us-has-killed-more-than-20-million-people-in-37-victim-nations-since-world-war-ii/5492051\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.prwatch.org/news/2017/01/13207/betsy-devos-ethics-report-reveals-ties-student-debt-collection-firm\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.snopes.com/white-house-comment-shutdown/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://ucr.fbi.gov/crime-in-the-u.s/2016/crime-in-the-u.s.-2016/tables/expanded-homicide-data-table-2.xls\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.latimes.com/opinion/op-ed/la-oe-goldberg-presidents-golf-louisiana-20160822-snap-story.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.nydailynews.com/new-york/trump-fought-legislation-requiring-sprinklers-nyc-buildings-article-1.3921642\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.m.wikipedia.org/wiki/Executive_Order_13535\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.reddit.com/r/ShitPoliticsSays/comments/7r6w2b/the_doctor_made_it_up_lmao_you_really_think_he/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://mrbricksworld.files.wordpress.com/2012/03/family-guy.gif\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=_lPJ9J-6vDw\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/UB_k_WiqtOg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://imgur.com/GWDcgSL\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.freedomworks.org/content/clinton’s-3-trillion-raid-social-security\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://time.com/3721452/hillary-clinton-net-neutrality/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=g_a7dQXilCo\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://twitter.com/RVAwonk/status/850589932502609920/photo/1\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://thehill.com/blogs/blog-briefing-room/398279-comey-to-dems-dont-lose-your-minds-and-rush-to-the-socialist-left\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.fostercampbell2016.com\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://thehill.com/homenews/senate/316583-mcconnell-all-but-rules-out-filibuster-change\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.slate.com/articles/news_and_politics/politics/2016/12/the_myth_of_the_rust_belt_revolt.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.statista.com/statistics/191219/reported-violent-crime-rate-in-the-usa-since-1990/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.politico.com/election-results/2018/house-senate-race-ratings-and-predictions/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.theguardian.com/books/booksblog/2017/sep/14/what-happened-10-books-by-failed-presidential-candidates-hillary-clinton\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://cbs12.com/news/local/black-stoneman-douglas-students-say-they-are-overlooked-by-movement-and-media\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=ZT9S9O-cvTI&amp;t=2s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://i.imgur.com/mwVPnEI.gif\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.wackyy.org/funny-squirrel-soldier/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://nymag.com/daily/intelligencer/2018/05/michael-cohen-bribery-scandal-is-now-a-trump-bribery-scandal.html\n",
      "\n",
      "\n",
      "https://www.theguardian.com/us-news/2018/may/28/ivanka-trump-won-china-trademarks-donald-trump-zte-reversal\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.indeed.com/m/jobs?q=&amp;l=Utah\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.snopes.com/trump-criticize-obama-shutdown/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.nbcnews.com/politics/elections/democrats-opposing-pelosi-n899536\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/gNodh9OlTrQ\n",
      "\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.ktsm.com/news/beto-orourke-veronica-escobar-lead-march-on-tent-city-housing-separated-immigrant-children/1245093728\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.thelibel.org/bomb-bomb-iran\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://i.imgur.com/nwIDkNX.jpg\n",
      "\n",
      "http://i.imgur.com/J1jBkzY.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://thehill.com/homenews/administration/334223-russian-government-promotes-conspiracy-surrounding-murdered-dnc\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.chicagotribune.com/news/columnists/kass/ct-hillary-clinton-open-borders-kass-1012-20161011-column.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.wikipedia.org/wiki/Civil_Disobedience_(Thoreau)\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=iUaLfjAeyO8&amp;feature=share\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.startribune.com/rep-ellison-hones-new-voter-turnout-strategy-for-democrats/363536691/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.m.wikipedia.org/wiki/Gun_show_loophole\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.washingtonpost.com/news/politics/wp/2018/06/01/trumps-spent-far-more-going-to-mar-a-lago-alone-than-the-mueller-probe-has-cost/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.vox.com/policy-and-politics/2018/2/16/17021248/russian-election-interference-sanders-stein-trump\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=glskOdbff68&amp;feature=youtu.be\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://quotesideas.com/wp-content/uploads/2015/11/Waiting-for-Santa-Merry-Christmas-quote.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://twitter.com/TrueFactsStated/status/855110244040667136\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.telegraph.co.uk/news/2016/08/02/how-donald-trump-avoided-the-draft-during-the-vietnam-war/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.lmfgtfy.com/?q=semi\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/tPPM58UtbHA\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/g37HT4-EtzE\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.reddit.com/r/democrats/comments/5uuz60/i_want_everyone_to_take_a_good_hard_look_at_this/ddx5mv1/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/F_3dmn42tms\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.nytimes.com/interactive/2017/11/06/opinion/how-to-reduce-shootings.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.sltrib.com/news/politics/2018/08/13/congressman-rob-bishop/\n",
      "\n",
      "https://ballotpedia.org/Rob_Bishop\n",
      "\n",
      "https://en.wikipedia.org/wiki/Rob_Bishop\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.npr.org/2018/06/11/618870982/supreme-court-upholds-controversial-ohio-voter-purge-law\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.huffingtonpost.com/entry/the-11-gun-bills-tulsi-gabbard-wont-supportwhile_us_57635ad1e4b0092652d744e9\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.cutestpaw.com/wp-content/uploads/2012/12/awe-so-cute.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://np.reddit.com/r/politics/comments/9des3v/sen_harris_asks_kavanaugh_if_he_discussed_the/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://giphy.com/gifs/pulp-fiction-john-travolta-i-made-a-thing-rRkVYvwfldv8c\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://imgflip.com/i/2dl6m7\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.rt.com/usa/408576-confederate-truck-children-ad-virginia/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.cnn.com/election/2016/results/exit-polls/national/president\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.wikipedia.org/wiki/Government_shutdowns_in_the_United_States#1980\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://act.indivisibleguide.com/event/stand-in-solidarity-with-charlottesville/search/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.wikipedia.org/wiki/Motion_of_no_confidence\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.intelligence.senate.gov/sites/default/files/documents/os-jcomey-060817.pdf\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://mobile.twitter.com/realDonaldTrump/status/966662241977360384\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://politic365.com/2012/01/27/the-10-worst-moments-of-disrespect-towards-president-obama/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.latimes.com/projects/la-na-mueller-investigation-one-year/\n",
      "\n",
      "https://www.vox.com/2018/5/23/17384096/mueller-investigation-poll\n",
      "\n",
      "http://www.foxnews.com/politics/2018/06/08/whos-been-charged-by-mueller-in-russia-probe-so-far.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.reddit.com/r/democrats/comments/8d0lz9/harry_truman_nails_the_republican_party_perfectly/dxjp5tg/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://vignette2.wikia.nocookie.net/spykids/images/3/36/Spybutts04.png/revision/latest?cb=20100711143300\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://imgur.com/Ka3shuO\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://thumbs.dreamstime.com/b/many-straw-hay-bales-stacked-big-pile-la-rioja-spain-33682023.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://np.reddit.com/r/BlueMidterm2018/comments/6vcwpi/itt_a_reformed_authoritarian_cultist_explains_how/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://2static4.fjcdn.com/thumbnails/comments/Stoner+humor+totally+not+retarded+_49c722210cfbf334fb7b1e1c4822f249.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.senate.gov/legislative/votes.htm\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://i.imgur.com/dNiKrBd.gifv\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.amazon.com/gp/aw/d/0812972724/ref=mp_s_a_1_1?ie=UTF8&amp;qid=1526744485&amp;sr=8-1&amp;pi=AC_SX236_SY340_FMwebp_QL65&amp;keywords=america%27s+constitution+a+biography&amp;dpPl=1&amp;dpID=51YZrTSyeLL&amp;ref=plSrch\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://imgur.com/gallery/agisn\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.wikipedia.org/wiki/Ad_hominem\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.psdgraphics.com/file/gold-trophy-cup.jpg\n",
      "\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.taxpolicycenter.org/briefing-book/what-paygo\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.spidermancrawlspace.com/wp-content/uploads/2015/11/JS75769970.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://i.imgur.com/25vg2JL.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.m.wikipedia.org/wiki/Gun-Free_School_Zones_Act_of_1990\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://thechristianleft.org\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://newrepublic.com/minutes/137245/trumps-400-pound-hacker-comments-not-go-well-4chan-supporters\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://m.youtube.com/watch?v=vPBrt-mdNmQ\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://pbs.twimg.com/media/Cp6oPfmWAAA1fX7.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.oii.ox.ac.uk/blog/russia-weaponized-twitter-to-sway-election/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://np.reddit.com/r/politics/comments/885c4m/hillary_clinton_fires_back_at_critics_no_one_told/\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.alexdems.org/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/zc4nPaaAKd0\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=ugyqOSUlR2A\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.wikipedia.org/wiki/American-led_intervention_in_the_Syrian_Civil_War\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://theintercept.imgix.net/wp-uploads/sites/1/2017/11/fischer-1509885375.png?auto=compress%2Cformat&amp;q=90\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://imgur.com/JCmnjFT\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://media.salon.com/2016/11/MMFA-emails-Bohelert.png\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=R1Nd6LU4ImQ\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.politifact.com/personalities/hillary-clinton/statements/byruling/false/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.reddit.com/r/Liberal/comments/7btwrv/why_are_conservatives_more_susceptible_to/dpkr8ar/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.google.com/amp/s/lawandcrime.com/high-profile/doctors-claim-they-saw-protesters-get-paid-to-disrupt-kavanaugh-hearing/amp/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://i.imgur.com/ltpN9mu.gifv\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://2c52x93oh4vy383tvsi7rkm1-wpengine.netdna-ssl.com/wp-content/uploads/2014/12/Clarification.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://imgur.com/t6Oh0PV\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.c-span.org/video/?435050-1/senators-burr-warner-hold-news-conference-russia-probe\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=0w6L93kD3xw\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.thecut.com/2018/04/donald-trump-pee-tape.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.washingtonpost.com/opinions/were-seth-richs-parents-stop-politicizing-our-sons-murder/2017/05/23/164cf4dc-3fee-11e7-9869-bac8b446820a_story.html?utm_term=.8a359789f6ca\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/yFNRlvEh7ok\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/ybDi6R8TXvI\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.governing.com/gov-data/safety-justice/gun-show-firearms-bankground-checks-state-laws-map.html\n",
      "\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://i.imgur.com/tWD6706.png\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/QV7dDSgbaQ0\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://s.yimg.com/ny/api/res/1.2/GVb7b22Uz1BSY0LMEqMu4g--/YXBwaWQ9aGlnaGxhbmRlcjtzbT0xO3c9NjM2O2g9Mzgy/http://l.yimg.com/yp/offnetwork/da1e1a4d9f2b3fa1a1dbfac47b62edb9\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://i.imgur.com/OaStfQF.jpg\n",
      "\n",
      "https://imgur.com/a/lIk0TuN\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.newyorker.com/magazine/2018/03/12/christopher-steele-the-man-behind-the-trump-dossier\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://imgur.com/ePP4xSm\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://np.reddit.com/r/democrats/comments/91uym9/centrism_is_dead_the_left_has_already_won_the/e31icxo/?context=3\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=wVckRJvuBQY\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=wooGSr7k1-s&amp;feature=youtu.be&amp;t=9s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.merriam-webster.com/dictionary/win-win\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.theatlantic.com/politics/archive/2016/09/donald-trumps-cruel-streak/501554/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://imgur.com/gallery/1VOlSLn\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.nytimes.com/2017/05/08/us/politics/obama-flynn-trump.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://imgur.com/Ob37D6h\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.wikipedia.org/wiki/German_American_Bund\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtube.com/watch?v=cryMVK1PwuQ\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/QL6sMfqz6wQ\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://np.reddit.com/r/politics/comments/782ri5/why_is_bill_browder_banned_from_america/doqty4o/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.cnn.com/2016/11/10/us/post-election-hate-crimes-and-fears-trnd/index.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.politifact.com/truth-o-meter/statements/2017/oct/20/john-kelly/fact-checking-john-kelly-frederica-wilsons-2015-sp/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://np.old.reddit.com/r/politics/comments/9des3v/sen_harris_asks_kavanaugh_if_he_discussed_the/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://sorb.chs.state.ma.us/sorbpublic/viewOffenderDetails.action?_p=aJESFbWwET_nCIQXpgWAMZ73TDEWDTbu\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://twitter.com/cameron_kasky/status/1019314229537529856\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.cnn.com/2017/10/10/politics/niger-deadly-ambush-us-intelligence/index.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/r3hTwsvJV_A\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/0YNQ2T6UTmo\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://m.imgur.com/a/O5xR3\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.wired.com/story/robert-mueller-vietnam/\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://s-media-cache-ak0.pinimg.com/736x/18/af/b0/18afb09636abba279dc2e3196d0f9b88--beard-art-men-beard.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://christineforvermont.com\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.msnbc.com/rachel-maddow/watch/kavanaugh-in-for-a-long-hard-fight-for-supreme-court-spot-1274368579570?v=railb\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://i.imgur.com/dUEyaR5.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/K1TGn0UfFpE\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://i.imgur.com/HUQ6gzX.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.google.com/amp/www.latimes.com/local/lanow/la-me-jails-ice-audit-20171009-story.html%3foutputType=amp\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://s3.amazonaws.com/lowres.cartoonstock.com/none-election-midterm-midterm_election-russian_hacker-russian_hackers-cwln8849_low.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/S9RVS8cjNN0\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://img.4plebs.org/boards/pol/image/1470/35/1470352686489.png\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://nymag.com/daily/intelligencer/2016/11/activists-urge-hillary-clinton-to-challenge-election-results.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.wikipedia.org/wiki/Beyond_Belief:_Fact_or_Fiction?wprov=sfti1\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://techcrunch.com/2017/09/11/study-finds-reddits-controversial-ban-of-its-most-toxic-subreddits-actually-worked/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=zTFp7WG9J-E\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.washingtonpost.com/posteverything/wp/2017/01/23/repealing-the-affordable-care-act-will-kill-more-than-43000-people-annually/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://media.giphy.com/media/VsXhOdCYnpw1q/giphy.gif\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://i.imgur.com/ilHmHW7.gif\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://lmgtfy.com/?q=March+for+our+lives+-+Washington%2C+DC\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/PGNiXGX2nLU\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.cookpolitical.com/ratings/house-race-ratings\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.nytimes.com/2017/03/25/business/alex-jones-pizzagate-apology-comet-ping-pong.html?_r=0\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.thebalance.com/us-deficit-by-year-3306306\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/N0IWe11RWOM\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.adweek.com/news/advertising-branding/adam-moss-earns-editor-year-guiding-ny-magazines-election-coverage-174483\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.theguardian.com/us-news/2017/apr/28/greg-gianforte-republican-candidate-congress-russia-companies\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://giant.gfycat.com/NippyKindLangur.gif\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://petitions.moveon.org/sign/children-under-the-age?source=c.em&amp;r_by=19721557\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://twitter.com/Emma4Change\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.independent.co.uk/news/business/news/ttip-trade-deal-new-what-is-tisa-privatisation-pact-secret-threat-to-democracy-a7216296.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.senate.gov/general/contacting.htm\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.m.wikipedia.org/wiki/List_of_massacres_in_Australia\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.washingtonpost.com/blogs/post-partisan/wp/2018/02/28/is-it-okay-to-criticize-the-parkland-kids/?noredirect=on&amp;utm_term=.6b157c20efb7\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/k4446wQ1dDU\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://static4.fjcdn.com/comments/Welp+that+settles+it+guess+we+re+rioting+_6ce838fd546ffd6ae4d350199b6a2527.png\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.merriam-webster.com/dictionary/pander\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.newbedfordguide.com/11-massachusetts-democrats-sponsor-bill-to-allow-non-citizens-to-vote-in-local-elections/2017/10/25\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.surveymonkey.com/mp/survey-methodology/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.familiesbelongtogether.org/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.m.wikipedia.org/wiki/Acetone_peroxide\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.nytimes.com/interactive/2018/01/15/opinion/leonhardt-trump-racist.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://morallaw.org/america-the-beautiful-by-judge-roy-moore/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://qz.com/1368251/black-income-is-half-that-of-white-households-just-like-it-was-in-the-1950s/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.merriam-webster.com/dictionary/war\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.vanityfair.com/news/2018/06/the-nra-spent-dollar30-million-to-elect-trump-was-it-russian-money\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.google.com/amp/s/www.vox.com/platform/amp/2018/4/13/17229018/undocumented-immigrants-pay-taxes\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.trumpnationalregistry.com/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.m.wikipedia.org/wiki/Watergate_scandal\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://web.archive.org/web/20170625035009/https://www.reddit.com/r/democrats/comments/6jat5r/sanders_sold_off_a_house_that_has_been_passed/\n",
      "\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://imgur.com/a/RqBux\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.bostonglobe.com/news/politics/2017/05/05/trump-has-been-sued-times-federal-court-since-inauguration-day/E4AqZBYaKYHtzwfQ3k9hdM/story.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.independent.co.uk/news/world/americas/us-elections/bernie-sanders-publishes-tax-returns-showing-205000-earnings-following-hillary-clintons-challenge-a6986951.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://slate.com/news-and-politics/2018/07/joe-crowley-denies-hes-waging-3rd-party-big-against-alexandria-ocasio-cortez.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.thedailybeast.com/new-poll-43-of-republicans-want-to-give-trump-the-power-to-shut-down-media\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.gq.com/story/nothing-is-the-same-as-racism\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.forbes.com/sites/joshmax/2017/10/10/comparing-gun-carnage-with-auto-deaths-is-a-flawed-argument/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://s-media-cache-ak0.pinimg.com/originals/e9/cd/99/e9cd99f8b68d27aa632d3d16280cd01c.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://en.wikipedia.org/wiki/Tom_Perez#Actions\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.google.ca/amp/s/www.cbsnews.com/amp/news/how-u-s-gun-deaths-compare-to-other-countries/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://youtu.be/_-9ov40Pm3Y\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "df['body'] = df.body.apply(review_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df_dem_rep_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 604793 entries, 0 to 604792\n",
      "Data columns (total 11 columns):\n",
      "subreddit      604788 non-null float64\n",
      "author         604788 non-null object\n",
      "score          604788 non-null float64\n",
      "created_utc    604788 non-null float64\n",
      "day_of_week    604788 non-null object\n",
      "month          604788 non-null object\n",
      "date           604788 non-null float64\n",
      "year           604788 non-null float64\n",
      "hour           604788 non-null float64\n",
      "minute         604788 non-null float64\n",
      "body           604793 non-null object\n",
      "dtypes: float64(7), object(4)\n",
      "memory usage: 50.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "#got 5 nulls somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[pd.isnull(df).any(axis=1)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit         0\n",
       "author            0\n",
       "score             0\n",
       "created_utc       0\n",
       "day_of_week       0\n",
       "month             0\n",
       "date              0\n",
       "year              0\n",
       "hour              0\n",
       "minute            0\n",
       "body           5525\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>waldrop02</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.490792e+09</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>March</td>\n",
       "      <td>29.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>rude person general eliminate idea rude higher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>nakdamink</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.508883e+09</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>October</td>\n",
       "      <td>24.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>fully agree moore hold absolutely reprehensibl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>HankMoodyMF</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.511795e+09</td>\n",
       "      <td>Monday</td>\n",
       "      <td>November</td>\n",
       "      <td>27.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>guy awful left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>mydadisnotyourdad</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.507227e+09</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>October</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>sure slam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>BobcatBarry</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.499877e+09</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>July</td>\n",
       "      <td>12.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>think destroys pretty strong word whatever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit             author  score   created_utc day_of_week     month  \\\n",
       "0        1.0          waldrop02   11.0  1.490792e+09   Wednesday     March   \n",
       "1        0.0          nakdamink    1.0  1.508883e+09     Tuesday   October   \n",
       "2        1.0        HankMoodyMF    1.0  1.511795e+09      Monday  November   \n",
       "3        0.0  mydadisnotyourdad    1.0  1.507227e+09    Thursday   October   \n",
       "4        0.0        BobcatBarry    9.0  1.499877e+09   Wednesday      July   \n",
       "\n",
       "   date  year  hour  minute                                               body  \n",
       "0  29.0  17.0   8.0    51.0  rude person general eliminate idea rude higher...  \n",
       "1  24.0  17.0   6.0     2.0  fully agree moore hold absolutely reprehensibl...  \n",
       "2  27.0  17.0   9.0    55.0                                     guy awful left  \n",
       "3   5.0  17.0   2.0    11.0                                          sure slam  \n",
       "4  12.0  17.0  12.0    23.0         think destroys pretty strong word whatever  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>waldrop02</td>\n",
       "      <td>11</td>\n",
       "      <td>1.490792e+09</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>March</td>\n",
       "      <td>29</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>51</td>\n",
       "      <td>rude person general eliminate idea rude higher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>nakdamink</td>\n",
       "      <td>1</td>\n",
       "      <td>1.508883e+09</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>October</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>fully agree moore hold absolutely reprehensibl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit     author  score   created_utc day_of_week    month  date  year  \\\n",
       "0          1  waldrop02     11  1.490792e+09   Wednesday    March    29    17   \n",
       "1          0  nakdamink      1  1.508883e+09     Tuesday  October    24    17   \n",
       "\n",
       "   hour  minute                                               body  \n",
       "0     8      51  rude person general eliminate idea rude higher...  \n",
       "1     6       2  fully agree moore hold absolutely reprehensibl...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df values must have encoded as floats when exporting/importing\n",
    "df.subreddit = df.subreddit.astype(int)\n",
    "df.score = df.score.astype(int)\n",
    "df.date = df.date.astype(int)\n",
    "df.year = df.year.astype(int)\n",
    "df.hour = df.hour.astype(int)\n",
    "df.minute = df.minute.astype(int)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df_dem_rep_comments.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating comments into individual words with count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will do this once without ngrams, once with ngrams(1,3)\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   #stopwords already removed\n",
    "                             max_features = 5000,\n",
    "                             max_df=1.0, #default\n",
    "                             min_df=1    #default\n",
    "                            )\n",
    "df_vect = vectorizer.fit_transform(df['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_words = pd.DataFrame(df_vect.todense(), columns=vectorizer.get_feature_names(), index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2996315000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_words.size  #6GB?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#was taking too long to export.  Probably don't need to encode and save a giant file like this.\n",
    "#df_words.to_csv('df_words.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count vectorizer with (3,3) ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_ng = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   #stopwords already removed\n",
    "                             max_features = 5000,\n",
    "                             ngram_range = (3,3),\n",
    "                             max_df=1.0, #default\n",
    "                             min_df=1    #default\n",
    "                            )\n",
    "df_vect_ng = vectorizer_ng.fit_transform(df['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_words_ng = pd.DataFrame(df_vect_ng.todense(), columns=vectorizer_ng.get_feature_names(), index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2996315000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_words_ng.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#was taking too long to export.  Probably don't need to encode and save a giant file like this.\n",
    "#df_words_ng.to_csv('df_words_ng.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcorpus = list(df.body[(df.subreddit ==1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcorpus = list(df.body[(df.subreddit ==0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='word', max_features=300)\n",
    "dem_tf = tfidf.fit_transform(dcorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_tf = pd.DataFrame(dem_tf.todense(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2432293107600088"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dem_tf.mean().sum()      #hmmmmm, thought it would be 1, or 2.7xxxx(euler's #)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trump         0.035459\n",
       "people        0.032827\n",
       "like          0.029966\n",
       "would         0.026215\n",
       "democrat      0.024385\n",
       "think         0.023376\n",
       "one           0.022949\n",
       "get           0.022807\n",
       "right         0.021139\n",
       "republican    0.020101\n",
       "http          0.019694\n",
       "party         0.018798\n",
       "vote          0.018767\n",
       "know          0.018023\n",
       "need          0.017372\n",
       "make          0.016513\n",
       "gt            0.016385\n",
       "even          0.016151\n",
       "say           0.016132\n",
       "thing         0.016111\n",
       "election      0.016091\n",
       "good          0.016050\n",
       "want          0.015903\n",
       "time          0.015689\n",
       "com           0.014779\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dem_tf.mean().sort_values(ascending=False)[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rtfidf = TfidfVectorizer(analyzer='word', max_features=300)\n",
    "rep_tf = Rtfidf.fit_transform(rcorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_tf = pd.DataFrame(rep_tf.todense(), columns=Rtfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.284270537989342"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rep_tf.mean().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trump         0.040294\n",
       "like          0.032391\n",
       "people        0.031002\n",
       "republican    0.030129\n",
       "one           0.021800\n",
       "would         0.021175\n",
       "http          0.020890\n",
       "right         0.020443\n",
       "get           0.019768\n",
       "think         0.019422\n",
       "know          0.018595\n",
       "com           0.018415\n",
       "president     0.018215\n",
       "gt            0.017870\n",
       "please        0.017741\n",
       "say           0.017034\n",
       "democrat      0.016907\n",
       "obama         0.016816\n",
       "good          0.016776\n",
       "thing         0.016509\n",
       "even          0.016207\n",
       "news          0.016059\n",
       "year          0.015087\n",
       "make          0.015045\n",
       "want          0.015040\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rep_tf.mean().sort_values(ascending=False)[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>waldrop02</td>\n",
       "      <td>11</td>\n",
       "      <td>1.490792e+09</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>March</td>\n",
       "      <td>29</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>51</td>\n",
       "      <td>rude person general eliminate idea rude higher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>nakdamink</td>\n",
       "      <td>1</td>\n",
       "      <td>1.508883e+09</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>October</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>fully agree moore hold absolutely reprehensibl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>HankMoodyMF</td>\n",
       "      <td>1</td>\n",
       "      <td>1.511795e+09</td>\n",
       "      <td>Monday</td>\n",
       "      <td>November</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>55</td>\n",
       "      <td>guy awful left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>mydadisnotyourdad</td>\n",
       "      <td>1</td>\n",
       "      <td>1.507227e+09</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>October</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>sure slam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>BobcatBarry</td>\n",
       "      <td>9</td>\n",
       "      <td>1.499877e+09</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>July</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>think destroys pretty strong word whatever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit             author  score   created_utc day_of_week     month  \\\n",
       "0          1          waldrop02     11  1.490792e+09   Wednesday     March   \n",
       "1          0          nakdamink      1  1.508883e+09     Tuesday   October   \n",
       "2          1        HankMoodyMF      1  1.511795e+09      Monday  November   \n",
       "3          0  mydadisnotyourdad      1  1.507227e+09    Thursday   October   \n",
       "4          0        BobcatBarry      9  1.499877e+09   Wednesday      July   \n",
       "\n",
       "   date  year  hour  minute                                               body  \n",
       "0    29    17     8      51  rude person general eliminate idea rude higher...  \n",
       "1    24    17     6       2  fully agree moore hold absolutely reprehensibl...  \n",
       "2    27    17     9      55                                     guy awful left  \n",
       "3     5    17     2      11                                          sure slam  \n",
       "4    12    17    12      23         think destroys pretty strong word whatever  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#kernel died from train_test_split\n",
    "df = pd.read_csv('./df_dem_rep_comments.csv')\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit      0\n",
       "author         0\n",
       "score          0\n",
       "created_utc    0\n",
       "day_of_week    0\n",
       "month          0\n",
       "date           0\n",
       "year           0\n",
       "hour           0\n",
       "minute         0\n",
       "body           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#somehow lose 5525 body during every export import to_csv/read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[pd.isnull(df).any(axis=1)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "#note, because the dataframe is over 600,000 rows, i'm going to split this 5%train 95%test\n",
    "#i think that this better characterizes 'real world' data problems, and 30,000 is still a good size to train on\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_vect, df['subreddit'], random_state=42, test_size=.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-RandomForestClassifier with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV] max_features=5, n_estimators=10 .................................\n",
      "[CV] max_features=5, n_estimators=10 .................................\n",
      "[CV] max_features=5, n_estimators=10 .................................\n",
      "[CV] max_features=5, n_estimators=100 ................................\n",
      "[CV]  max_features=5, n_estimators=10, score=0.8571285542651181, total=   3.2s\n",
      "[CV] max_features=5, n_estimators=100 ................................\n",
      "[CV]  max_features=5, n_estimators=10, score=0.8580296355626752, total=   3.3s\n",
      "[CV] max_features=5, n_estimators=100 ................................\n",
      "[CV]  max_features=5, n_estimators=10, score=0.8520076098928607, total=   3.3s\n",
      "[CV] max_features=5, n_estimators=1000 ...............................\n",
      "[CV]  max_features=5, n_estimators=100, score=0.8986784140969163, total=  34.2s\n",
      "[CV] max_features=5, n_estimators=1000 ...............................\n",
      "[CV]  max_features=5, n_estimators=100, score=0.900170221287674, total=  34.2s\n",
      "[CV] max_features=5, n_estimators=1000 ...............................\n",
      "[CV]  max_features=5, n_estimators=100, score=0.9042851421706047, total=  34.3s\n",
      "[CV] max_features=10, n_estimators=10 ................................\n",
      "[CV]  max_features=10, n_estimators=10, score=0.8490188225871045, total=   3.2s\n",
      "[CV] max_features=10, n_estimators=10 ................................\n",
      "[CV]  max_features=10, n_estimators=10, score=0.8578293952743292, total=   3.3s\n",
      "[CV] max_features=10, n_estimators=10 ................................\n",
      "[CV]  max_features=10, n_estimators=10, score=0.8518073495544207, total=   3.5s\n",
      "[CV] max_features=10, n_estimators=100 ...............................\n",
      "[CV]  max_features=10, n_estimators=100, score=0.8953744493392071, total=  32.6s\n",
      "[CV] max_features=10, n_estimators=100 ...............................\n",
      "[CV]  max_features=10, n_estimators=100, score=0.9009811774128955, total=  33.1s\n",
      "[CV] max_features=10, n_estimators=100 ...............................\n",
      "[CV]  max_features=10, n_estimators=100, score=0.8942625413036948, total=  33.3s\n",
      "[CV] max_features=10, n_estimators=1000 ..............................\n",
      "[CV]  max_features=5, n_estimators=1000, score=0.9039847817380857, total= 6.0min\n",
      "[CV] max_features=10, n_estimators=1000 ..............................\n",
      "[CV]  max_features=5, n_estimators=1000, score=0.9088906688025631, total= 5.9min\n",
      "[CV] max_features=10, n_estimators=1000 ..............................\n",
      "[CV]  max_features=5, n_estimators=1000, score=0.9029738660258336, total= 5.9min\n",
      "[CV] max_features=50, n_estimators=10 ................................\n",
      "[CV]  max_features=50, n_estimators=10, score=0.8448137765318382, total=   2.9s\n",
      "[CV] max_features=50, n_estimators=10 ................................\n",
      "[CV]  max_features=50, n_estimators=10, score=0.8481177412895474, total=   3.0s\n",
      "[CV] max_features=50, n_estimators=10 ................................\n",
      "[CV]  max_features=50, n_estimators=10, score=0.8406929007710023, total=   3.0s\n",
      "[CV] max_features=50, n_estimators=100 ...............................\n",
      "[CV]  max_features=50, n_estimators=100, score=0.882258710452543, total=  29.8s\n",
      "[CV] max_features=50, n_estimators=100 ...............................\n",
      "[CV]  max_features=50, n_estimators=100, score=0.8884661593912695, total=  29.5s\n",
      "[CV] max_features=50, n_estimators=100 ...............................\n",
      "[CV]  max_features=10, n_estimators=1000, score=0.9008810572687225, total= 5.3min\n",
      "[CV] max_features=50, n_estimators=1000 ..............................\n",
      "[CV]  max_features=50, n_estimators=100, score=0.877841193551617, total=  31.5s\n",
      "[CV] max_features=50, n_estimators=1000 ..............................\n",
      "[CV]  max_features=10, n_estimators=1000, score=0.9051862234681618, total= 5.4min\n",
      "[CV] max_features=50, n_estimators=1000 ..............................\n",
      "[CV]  max_features=10, n_estimators=1000, score=0.8965655351957544, total= 5.4min\n",
      "[CV] max_features=100, n_estimators=10 ...............................\n",
      "[CV]  max_features=100, n_estimators=10, score=0.8430116139367241, total=   3.2s\n",
      "[CV] max_features=100, n_estimators=10 ...............................\n",
      "[CV]  max_features=100, n_estimators=10, score=0.8456147376852222, total=   3.3s\n",
      "[CV] max_features=100, n_estimators=10 ...............................\n",
      "[CV]  max_features=100, n_estimators=10, score=0.8401922499249024, total=   3.3s\n",
      "[CV] max_features=100, n_estimators=100 ..............................\n",
      "[CV]  max_features=100, n_estimators=100, score=0.8705446535843011, total=  31.9s\n",
      "[CV] max_features=100, n_estimators=100 ..............................\n",
      "[CV]  max_features=50, n_estimators=1000, score=0.8857629154985983, total= 5.1min\n",
      "[CV] max_features=100, n_estimators=100 ..............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  29 tasks      | elapsed: 14.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_features=50, n_estimators=1000, score=0.8910692831397677, total= 5.0min\n",
      "[CV] max_features=100, n_estimators=1000 .............................\n",
      "[CV]  max_features=100, n_estimators=100, score=0.8789547456948338, total=  32.4s\n",
      "[CV] max_features=100, n_estimators=1000 .............................\n",
      "[CV]  max_features=100, n_estimators=100, score=0.8684289576449384, total=  32.0s\n",
      "[CV] max_features=100, n_estimators=1000 .............................\n",
      "[CV]  max_features=50, n_estimators=1000, score=0.8817462701511966, total= 4.9min\n",
      "[CV]  max_features=100, n_estimators=1000, score=0.8731350755982777, total= 5.3min\n",
      "[CV]  max_features=100, n_estimators=1000, score=0.8747496996395675, total= 5.4min\n",
      "[CV]  max_features=100, n_estimators=1000, score=0.8837605126151382, total= 5.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  36 out of  36 | elapsed: 21.3min finished\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0ee2301c75e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mgs_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    737\u001b[0m                 **self.best_params_)\n\u001b[1;32m    738\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 328\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid_params = {\n",
    "    'n_estimators' : [10, 100, 1000, 2000],\n",
    "    'max_features': [5, 10, 50, 100]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    grid_params,\n",
    "    verbose = 2.5,\n",
    "    n_jobs = 4\n",
    ")\n",
    "\n",
    "gs_results = gs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I watched all of the outputs through verbose and noted that 5 features with 1000 estimators was the best model.  Lower features and higher estimators were always a better score over 36 attempts (3 * 4 * 3 folds).  Didn't try higher estimators in GridSearchCV, but will experiment below with 2000 estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=5, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_jobs=-1, n_estimators=2000, max_features=5)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9868504488869606"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.942352011241876"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test_hat = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These scores seem really good.  over 94% on test data with a 5/95 train-test-split.  I'd be happy with this as is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes = MultinomialNB()\n",
    "bayes.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7199212361913027"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6966959423853856"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat_bayes = bayes.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing alpha hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7232920602075894, 0.6991779378183735)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes = MultinomialNB(alpha=.01)\n",
    "bayes.fit(X_train, y_train)\n",
    "bayes.score(X_train, y_train), bayes.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7225911958081634, 0.6985807131565079)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes = MultinomialNB(alpha=.1)\n",
    "bayes.fit(X_train, y_train)\n",
    "bayes.score(X_train, y_train), bayes.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7028001201481827, 0.6818426137361672)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes = MultinomialNB(alpha=10)\n",
    "bayes.fit(X_train, y_train)\n",
    "bayes.score(X_train, y_train), bayes.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6726295764776558, 0.6587019146320042)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes = MultinomialNB(alpha=100)\n",
    "bayes.fit(X_train, y_train)\n",
    "bayes.score(X_train, y_train), bayes.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slight improvement with a lowered alpha, but still not good results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores are very low compared to the RandomForest.  I wouldn't even want to include it in a voting classifier.\n",
    "It is worth mentioning that the results from Multinomial Naïve Bayes are instant!  The RandomForestClassifier ran for hours.  Still, I wouldn't choose this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[CV] C=0.1, penalty=l1, solver=saga ..................................\n",
      "[CV] C=0.1, penalty=l1, solver=saga ..................................\n",
      "[CV] C=0.1, penalty=l1, solver=saga ..................................\n",
      "[CV] C=0.1, penalty=l2, solver=saga ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.1, penalty=l2, solver=saga, score=0.6954345214257108, total=   1.0s\n",
      "[CV] C=0.1, penalty=l2, solver=saga ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.1, penalty=l2, solver=saga, score=0.7019423307969563, total=   1.0s\n",
      "[CV] C=0.1, penalty=l2, solver=saga ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.1, penalty=l2, solver=saga, score=0.6868929608491038, total=   0.9s\n",
      "[CV] C=1.0, penalty=l1, solver=saga ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.1, penalty=l1, solver=saga, score=0.6359631557869443, total=   6.5s\n",
      "[CV] C=1.0, penalty=l1, solver=saga ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.1, penalty=l1, solver=saga, score=0.6345248823470512, total=   6.6s\n",
      "[CV] C=1.0, penalty=l1, solver=saga ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.1, penalty=l1, solver=saga, score=0.6416700040048058, total=   6.7s\n",
      "[CV] C=1.0, penalty=l2, solver=saga ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1.0, penalty=l2, solver=saga, score=0.6993392070484582, total=   0.9s\n",
      "[CV] C=1.0, penalty=l2, solver=saga ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1.0, penalty=l2, solver=saga, score=0.7035442531037245, total=   0.9s\n",
      "[CV] C=1.0, penalty=l2, solver=saga ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1.0, penalty=l2, solver=saga, score=0.6907980374486833, total=   1.1s\n",
      "[CV] C=10, penalty=l1, solver=saga ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1.0, penalty=l1, solver=saga, score=0.6895274329195034, total=  41.2s\n",
      "[CV] C=10, penalty=l1, solver=saga ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1.0, penalty=l1, solver=saga, score=0.697236684020825, total=  41.6s\n",
      "[CV] C=10, penalty=l1, solver=saga ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1.0, penalty=l1, solver=saga, score=0.6793831981576048, total=  41.5s\n",
      "[CV] C=10, penalty=l2, solver=saga ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, penalty=l2, solver=saga, score=0.6998398077693232, total=   1.0s\n",
      "[CV] C=10, penalty=l2, solver=saga ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, penalty=l2, solver=saga, score=0.7028434120945134, total=   1.1s\n",
      "[CV] C=10, penalty=l2, solver=saga ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, penalty=l2, solver=saga, score=0.6907980374486833, total=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, penalty=l1, solver=saga, score=0.697336804164998, total=  53.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-14:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/erikgreenj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/erikgreenj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/erikgreenj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/erikgreenj/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/erikgreenj/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/erikgreenj/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/pool.py\", line 362, in get\n",
      "    return recv()\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/erikgreenj/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/erikgreenj/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/erikgreenj/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-dcd5a229d933>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mgs_log_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#note that we can't run other solvers through gridsearch if we want to try both Lasso and Ridge regularization.\n",
    "#some solvers only work with Ridge.\n",
    "\n",
    "grid_params = {\n",
    "    'solver' : ['saga'],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C' : [.1, 1.0, 10]\n",
    "}\n",
    "\n",
    "gs_log = GridSearchCV(\n",
    "    LogisticRegression(),\n",
    "    grid_params,\n",
    "    verbose = 2.5,\n",
    "    n_jobs = 4\n",
    ")\n",
    "\n",
    "gs_log_results = gs_log.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interrupted cell, wouldn't terminate on it's own. Best score was .703"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Will try with different solvers now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "[CV] C=0.1, max_iter=100, penalty=l2, solver=sag .....................\n",
      "[CV] C=0.1, max_iter=100, penalty=l2, solver=sag .....................\n",
      "[CV] C=0.1, max_iter=100, penalty=l2, solver=sag .....................\n",
      "[CV] C=0.1, max_iter=100, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=0.1, max_iter=100, penalty=l2, solver=lbfgs, score=0.7247697236684021, total=   0.4s\n",
      "[CV] C=0.1, max_iter=100, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=0.1, max_iter=100, penalty=l2, solver=lbfgs, score=0.7286744092911493, total=   0.3s\n",
      "[CV] C=0.1, max_iter=100, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=0.1, max_iter=100, penalty=l2, solver=lbfgs, score=0.7190347451687193, total=   0.3s\n",
      "[CV] C=0.1, max_iter=100, penalty=l2, solver=newton-cg ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.1, max_iter=100, penalty=l2, solver=sag, score=0.7038149594472815, total=   1.3s\n",
      "[CV]  C=0.1, max_iter=100, penalty=l2, solver=sag, score=0.710853023628354, total=   1.4s\n",
      "[CV] C=0.1, max_iter=100, penalty=l2, solver=newton-cg ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] C=0.1, max_iter=100, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=0.1, max_iter=100, penalty=l2, solver=sag, score=0.7137565078093713, total=   1.4s\n",
      "[CV] C=0.1, max_iter=500, penalty=l2, solver=sag .....................\n",
      "[CV]  C=0.1, max_iter=100, penalty=l2, solver=newton-cg, score=0.7247697236684021, total=   0.6s\n",
      "[CV] C=0.1, max_iter=500, penalty=l2, solver=sag .....................\n",
      "[CV]  C=0.1, max_iter=100, penalty=l2, solver=newton-cg, score=0.7190347451687193, total=   0.6s\n",
      "[CV] C=0.1, max_iter=500, penalty=l2, solver=sag .....................\n",
      "[CV]  C=0.1, max_iter=100, penalty=l2, solver=newton-cg, score=0.7286744092911493, total=   0.6s\n",
      "[CV] C=0.1, max_iter=500, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=0.1, max_iter=500, penalty=l2, solver=lbfgs, score=0.7247697236684021, total=   0.3s\n",
      "[CV] C=0.1, max_iter=500, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=0.1, max_iter=500, penalty=l2, solver=lbfgs, score=0.7286744092911493, total=   0.3s\n",
      "[CV] C=0.1, max_iter=500, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=0.1, max_iter=500, penalty=l2, solver=lbfgs, score=0.7190347451687193, total=   0.3s\n",
      "[CV] C=0.1, max_iter=500, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=0.1, max_iter=500, penalty=l2, solver=newton-cg, score=0.7247697236684021, total=   0.5s\n",
      "[CV] C=0.1, max_iter=500, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=0.1, max_iter=500, penalty=l2, solver=newton-cg, score=0.7286744092911493, total=   0.5s\n",
      "[CV] C=0.1, max_iter=500, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=0.1, max_iter=500, penalty=l2, solver=newton-cg, score=0.7190347451687193, total=   0.5s\n",
      "[CV] C=0.1, max_iter=1000, penalty=l2, solver=sag ....................\n",
      "[CV]  C=0.1, max_iter=500, penalty=l2, solver=sag, score=0.723568281938326, total=   5.2s\n",
      "[CV] C=0.1, max_iter=1000, penalty=l2, solver=sag ....................\n",
      "[CV]  C=0.1, max_iter=500, penalty=l2, solver=sag, score=0.7282739287144574, total=   5.3s\n",
      "[CV] C=0.1, max_iter=1000, penalty=l2, solver=sag ....................\n",
      "[CV]  C=0.1, max_iter=500, penalty=l2, solver=sag, score=0.7188344848302793, total=   5.2s\n",
      "[CV] C=0.1, max_iter=1000, penalty=l2, solver=lbfgs ..................\n",
      "[CV]  C=0.1, max_iter=1000, penalty=l2, solver=lbfgs, score=0.7247697236684021, total=   0.3s\n",
      "[CV] C=0.1, max_iter=1000, penalty=l2, solver=lbfgs ..................\n",
      "[CV]  C=0.1, max_iter=1000, penalty=l2, solver=lbfgs, score=0.7286744092911493, total=   0.3s\n",
      "[CV] C=0.1, max_iter=1000, penalty=l2, solver=lbfgs ..................\n",
      "[CV]  C=0.1, max_iter=1000, penalty=l2, solver=lbfgs, score=0.7190347451687193, total=   0.3s\n",
      "[CV] C=0.1, max_iter=1000, penalty=l2, solver=newton-cg ..............\n",
      "[CV]  C=0.1, max_iter=1000, penalty=l2, solver=newton-cg, score=0.7247697236684021, total=   0.6s\n",
      "[CV] C=0.1, max_iter=1000, penalty=l2, solver=newton-cg ..............\n",
      "[CV]  C=0.1, max_iter=1000, penalty=l2, solver=newton-cg, score=0.7286744092911493, total=   0.5s\n",
      "[CV] C=0.1, max_iter=1000, penalty=l2, solver=newton-cg ..............\n",
      "[CV]  C=0.1, max_iter=1000, penalty=l2, solver=sag, score=0.723568281938326, total=   5.1s\n",
      "[CV] C=1.0, max_iter=100, penalty=l2, solver=sag .....................\n",
      "[CV]  C=0.1, max_iter=1000, penalty=l2, solver=newton-cg, score=0.7190347451687193, total=   0.5s\n",
      "[CV] C=1.0, max_iter=100, penalty=l2, solver=sag .....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1.0, max_iter=100, penalty=l2, solver=sag, score=0.7149579495394474, total=   1.1s\n",
      "[CV] C=1.0, max_iter=100, penalty=l2, solver=sag .....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1.0, max_iter=100, penalty=l2, solver=sag, score=0.7196635963155787, total=   1.1s\n",
      "[CV] C=1.0, max_iter=100, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=1.0, max_iter=100, penalty=l2, solver=lbfgs, score=0.7611133360032039, total=   0.4s\n",
      "[CV] C=1.0, max_iter=100, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=1.0, max_iter=100, penalty=l2, solver=lbfgs, score=0.7604124949939928, total=   0.4s\n",
      "[CV] C=1.0, max_iter=100, penalty=l2, solver=lbfgs ...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  29 tasks      | elapsed:   11.9s\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1.0, max_iter=100, penalty=l2, solver=sag, score=0.707019124862321, total=   1.4s\n",
      "[CV] C=1.0, max_iter=100, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=1.0, max_iter=100, penalty=l2, solver=lbfgs, score=0.7521778311805347, total=   0.5s\n",
      "[CV] C=1.0, max_iter=100, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=0.1, max_iter=1000, penalty=l2, solver=sag, score=0.7285742891469764, total=   6.4s\n",
      "[CV] C=1.0, max_iter=100, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=1.0, max_iter=100, penalty=l2, solver=newton-cg, score=0.7611133360032039, total=   0.9s\n",
      "[CV] C=1.0, max_iter=500, penalty=l2, solver=sag .....................\n",
      "[CV]  C=1.0, max_iter=100, penalty=l2, solver=newton-cg, score=0.7601121345614738, total=   0.9s\n",
      "[CV] C=1.0, max_iter=500, penalty=l2, solver=sag .....................\n",
      "[CV]  C=0.1, max_iter=1000, penalty=l2, solver=sag, score=0.7188344848302793, total=   6.6s\n",
      "[CV] C=1.0, max_iter=500, penalty=l2, solver=sag .....................\n",
      "[CV]  C=1.0, max_iter=100, penalty=l2, solver=newton-cg, score=0.7519775708420947, total=   1.0s\n",
      "[CV] C=1.0, max_iter=500, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=1.0, max_iter=500, penalty=l2, solver=lbfgs, score=0.7611133360032039, total=   0.8s\n",
      "[CV] C=1.0, max_iter=500, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=1.0, max_iter=500, penalty=l2, solver=lbfgs, score=0.7601121345614738, total=   0.7s\n",
      "[CV] C=1.0, max_iter=500, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=1.0, max_iter=500, penalty=l2, solver=lbfgs, score=0.7519775708420947, total=   0.7s\n",
      "[CV] C=1.0, max_iter=500, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=1.0, max_iter=500, penalty=l2, solver=newton-cg, score=0.7611133360032039, total=   0.8s\n",
      "[CV] C=1.0, max_iter=500, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=1.0, max_iter=500, penalty=l2, solver=newton-cg, score=0.7601121345614738, total=   0.7s\n",
      "[CV] C=1.0, max_iter=500, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=1.0, max_iter=500, penalty=l2, solver=newton-cg, score=0.7519775708420947, total=   0.9s\n",
      "[CV] C=1.0, max_iter=1000, penalty=l2, solver=sag ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1.0, max_iter=500, penalty=l2, solver=sag, score=0.7438926712054466, total=   6.6s\n",
      "[CV]  C=1.0, max_iter=500, penalty=l2, solver=sag, score=0.7423908690428515, total=   6.5s\n",
      "[CV] C=1.0, max_iter=1000, penalty=l2, solver=sag ....................\n",
      "[CV] C=1.0, max_iter=1000, penalty=l2, solver=sag ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1.0, max_iter=500, penalty=l2, solver=sag, score=0.735255832582357, total=   6.5s\n",
      "[CV] C=1.0, max_iter=1000, penalty=l2, solver=lbfgs ..................\n",
      "[CV]  C=1.0, max_iter=1000, penalty=l2, solver=lbfgs, score=0.7611133360032039, total=   0.8s\n",
      "[CV] C=1.0, max_iter=1000, penalty=l2, solver=lbfgs ..................\n",
      "[CV]  C=1.0, max_iter=1000, penalty=l2, solver=lbfgs, score=0.7601121345614738, total=   0.8s\n",
      "[CV] C=1.0, max_iter=1000, penalty=l2, solver=lbfgs ..................\n",
      "[CV]  C=1.0, max_iter=1000, penalty=l2, solver=lbfgs, score=0.7519775708420947, total=   0.8s\n",
      "[CV] C=1.0, max_iter=1000, penalty=l2, solver=newton-cg ..............\n",
      "[CV]  C=1.0, max_iter=1000, penalty=l2, solver=newton-cg, score=0.7611133360032039, total=   0.8s\n",
      "[CV] C=1.0, max_iter=1000, penalty=l2, solver=newton-cg ..............\n",
      "[CV]  C=1.0, max_iter=1000, penalty=l2, solver=newton-cg, score=0.7601121345614738, total=   0.7s\n",
      "[CV] C=1.0, max_iter=1000, penalty=l2, solver=newton-cg ..............\n",
      "[CV]  C=1.0, max_iter=1000, penalty=l2, solver=newton-cg, score=0.7519775708420947, total=   0.9s\n",
      "[CV] C=10, max_iter=100, penalty=l2, solver=sag ......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, max_iter=100, penalty=l2, solver=sag, score=0.7154585502603124, total=   1.0s\n",
      "[CV] C=10, max_iter=100, penalty=l2, solver=sag ......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, max_iter=100, penalty=l2, solver=sag, score=0.7209651581898278, total=   1.0s\n",
      "[CV] C=10, max_iter=100, penalty=l2, solver=sag ......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, max_iter=100, penalty=l2, solver=sag, score=0.707419645539201, total=   1.0s\n",
      "[CV] C=10, max_iter=100, penalty=l2, solver=lbfgs ....................\n",
      "[CV]  C=10, max_iter=100, penalty=l2, solver=lbfgs, score=0.775330396475771, total=   0.4s\n",
      "[CV] C=10, max_iter=100, penalty=l2, solver=lbfgs ....................\n",
      "[CV]  C=10, max_iter=100, penalty=l2, solver=lbfgs, score=0.7789347216659992, total=   0.4s\n",
      "[CV] C=10, max_iter=100, penalty=l2, solver=lbfgs ....................\n",
      "[CV]  C=10, max_iter=100, penalty=l2, solver=lbfgs, score=0.7698007409632522, total=   0.4s\n",
      "[CV] C=10, max_iter=100, penalty=l2, solver=newton-cg ................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1.0, max_iter=1000, penalty=l2, solver=sag, score=0.7514016820184221, total=  12.2s\n",
      "[CV] C=10, max_iter=100, penalty=l2, solver=newton-cg ................\n",
      "[CV]  C=10, max_iter=100, penalty=l2, solver=newton-cg, score=0.776531838205847, total=   1.8s\n",
      "[CV] C=10, max_iter=100, penalty=l2, solver=newton-cg ................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1.0, max_iter=1000, penalty=l2, solver=sag, score=0.7423650745969761, total=  12.8s\n",
      "[CV] C=10, max_iter=500, penalty=l2, solver=sag ......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1.0, max_iter=1000, penalty=l2, solver=sag, score=0.7551061273528233, total=  13.1s\n",
      "[CV] C=10, max_iter=500, penalty=l2, solver=sag ......................\n",
      "[CV]  C=10, max_iter=100, penalty=l2, solver=newton-cg, score=0.7829395274329195, total=   2.0s\n",
      "[CV] C=10, max_iter=500, penalty=l2, solver=sag ......................\n",
      "[CV]  C=10, max_iter=100, penalty=l2, solver=newton-cg, score=0.7725042555321918, total=   2.2s\n",
      "[CV] C=10, max_iter=500, penalty=l2, solver=lbfgs ....................\n",
      "[CV]  C=10, max_iter=500, penalty=l2, solver=lbfgs, score=0.776531838205847, total=   1.6s\n",
      "[CV] C=10, max_iter=500, penalty=l2, solver=lbfgs ....................\n",
      "[CV]  C=10, max_iter=500, penalty=l2, solver=lbfgs, score=0.7826391670004005, total=   1.7s\n",
      "[CV] C=10, max_iter=500, penalty=l2, solver=lbfgs ....................\n",
      "[CV]  C=10, max_iter=500, penalty=l2, solver=lbfgs, score=0.7725042555321918, total=   1.7s\n",
      "[CV] C=10, max_iter=500, penalty=l2, solver=newton-cg ................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, max_iter=500, penalty=l2, solver=sag, score=0.7443932719263116, total=   7.1s\n",
      "[CV] C=10, max_iter=500, penalty=l2, solver=newton-cg ................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, max_iter=500, penalty=l2, solver=sag, score=0.7464957949539447, total=   7.0s\n",
      "[CV] C=10, max_iter=500, penalty=l2, solver=newton-cg ................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, max_iter=500, penalty=l2, solver=sag, score=0.7366576549514369, total=   7.0s\n",
      "[CV] C=10, max_iter=1000, penalty=l2, solver=sag .....................\n",
      "[CV]  C=10, max_iter=500, penalty=l2, solver=newton-cg, score=0.776531838205847, total=   2.0s\n",
      "[CV] C=10, max_iter=1000, penalty=l2, solver=sag .....................\n",
      "[CV]  C=10, max_iter=500, penalty=l2, solver=newton-cg, score=0.7829395274329195, total=   2.1s\n",
      "[CV] C=10, max_iter=1000, penalty=l2, solver=sag .....................\n",
      "[CV]  C=10, max_iter=500, penalty=l2, solver=newton-cg, score=0.7725042555321918, total=   2.3s\n",
      "[CV] C=10, max_iter=1000, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=10, max_iter=1000, penalty=l2, solver=lbfgs, score=0.776531838205847, total=   1.7s\n",
      "[CV] C=10, max_iter=1000, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=10, max_iter=1000, penalty=l2, solver=lbfgs, score=0.7826391670004005, total=   1.9s\n",
      "[CV] C=10, max_iter=1000, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=10, max_iter=1000, penalty=l2, solver=lbfgs, score=0.7725042555321918, total=   1.8s\n",
      "[CV] C=10, max_iter=1000, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=10, max_iter=1000, penalty=l2, solver=newton-cg, score=0.776531838205847, total=   2.1s\n",
      "[CV] C=10, max_iter=1000, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=10, max_iter=1000, penalty=l2, solver=newton-cg, score=0.7829395274329195, total=   1.7s\n",
      "[CV] C=10, max_iter=1000, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=10, max_iter=1000, penalty=l2, solver=newton-cg, score=0.7725042555321918, total=   2.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, max_iter=1000, penalty=l2, solver=sag, score=0.7561073287945534, total=  14.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, max_iter=1000, penalty=l2, solver=sag, score=0.7558069683620344, total=  14.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, max_iter=1000, penalty=l2, solver=sag, score=0.7462701511965555, total=  14.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  81 out of  81 | elapsed:   55.9s finished\n"
     ]
    }
   ],
   "source": [
    "grid_params = {\n",
    "    'solver' : ['sag', 'lbfgs', 'newton-cg'],\n",
    "    'penalty': ['l2'],\n",
    "    'C' : [.1, 1.0, 10],\n",
    "    'max_iter' : [100, 500, 1000]\n",
    "}\n",
    "\n",
    "gs_log = GridSearchCV(\n",
    "    LogisticRegression(),\n",
    "    grid_params,\n",
    "    verbose = 2.5,\n",
    "    n_jobs = 4\n",
    ")\n",
    "\n",
    "gs_log_results = gs_log.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'C': 10, 'max_iter': 100, 'penalty': 'l2', 'solver': 'newton-cg'},\n",
       " 0.7773253679538097)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_log.best_params_, gs_log.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Noticed an improvement with these solvers, best score was .777<br>\n",
    "##### Maybe could get a better score with higher iters, because many of the tests did not converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "[CV] C=0.1, max_iter=100, penalty=l2, solver=sag .....................\n",
      "[CV] C=0.1, max_iter=100, penalty=l2, solver=sag .....................\n",
      "[CV] C=0.1, max_iter=100, penalty=l2, solver=sag .....................\n",
      "[CV] C=0.1, max_iter=100, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=0.1, max_iter=100, penalty=l2, solver=lbfgs, score=0.7247697236684021, total=   0.4s\n",
      "[CV] C=0.1, max_iter=100, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=0.1, max_iter=100, penalty=l2, solver=lbfgs, score=0.7286744092911493, total=   0.3s\n",
      "[CV] C=0.1, max_iter=100, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=0.1, max_iter=100, penalty=l2, solver=lbfgs, score=0.7190347451687193, total=   0.3s\n",
      "[CV] C=0.1, max_iter=100, penalty=l2, solver=newton-cg ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.1, max_iter=100, penalty=l2, solver=sag, score=0.710853023628354, total=   1.3s\n",
      "[CV]  C=0.1, max_iter=100, penalty=l2, solver=sag, score=0.7038149594472815, total=   1.3s\n",
      "[CV] C=0.1, max_iter=100, penalty=l2, solver=newton-cg ...............\n",
      "[CV] C=0.1, max_iter=100, penalty=l2, solver=newton-cg ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.1, max_iter=100, penalty=l2, solver=sag, score=0.7137565078093713, total=   1.4s\n",
      "[CV] C=0.1, max_iter=500, penalty=l2, solver=sag .....................\n",
      "[CV]  C=0.1, max_iter=100, penalty=l2, solver=newton-cg, score=0.7247697236684021, total=   0.5s\n",
      "[CV] C=0.1, max_iter=500, penalty=l2, solver=sag .....................\n",
      "[CV]  C=0.1, max_iter=100, penalty=l2, solver=newton-cg, score=0.7190347451687193, total=   0.6s\n",
      "[CV] C=0.1, max_iter=500, penalty=l2, solver=sag .....................\n",
      "[CV]  C=0.1, max_iter=100, penalty=l2, solver=newton-cg, score=0.7286744092911493, total=   0.6s\n",
      "[CV] C=0.1, max_iter=500, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=0.1, max_iter=500, penalty=l2, solver=lbfgs, score=0.7247697236684021, total=   0.3s\n",
      "[CV] C=0.1, max_iter=500, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=0.1, max_iter=500, penalty=l2, solver=lbfgs, score=0.7286744092911493, total=   0.3s\n",
      "[CV] C=0.1, max_iter=500, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=0.1, max_iter=500, penalty=l2, solver=lbfgs, score=0.7190347451687193, total=   0.3s\n",
      "[CV] C=0.1, max_iter=500, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=0.1, max_iter=500, penalty=l2, solver=newton-cg, score=0.7247697236684021, total=   0.5s\n",
      "[CV] C=0.1, max_iter=500, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=0.1, max_iter=500, penalty=l2, solver=newton-cg, score=0.7286744092911493, total=   0.5s\n",
      "[CV] C=0.1, max_iter=500, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=0.1, max_iter=500, penalty=l2, solver=newton-cg, score=0.7190347451687193, total=   0.5s\n",
      "[CV] C=0.1, max_iter=1000, penalty=l2, solver=sag ....................\n",
      "[CV]  C=0.1, max_iter=500, penalty=l2, solver=sag, score=0.723568281938326, total=   5.2s\n",
      "[CV] C=0.1, max_iter=1000, penalty=l2, solver=sag ....................\n",
      "[CV]  C=0.1, max_iter=500, penalty=l2, solver=sag, score=0.7282739287144574, total=   5.3s\n",
      "[CV] C=0.1, max_iter=1000, penalty=l2, solver=sag ....................\n",
      "[CV]  C=0.1, max_iter=500, penalty=l2, solver=sag, score=0.7188344848302793, total=   5.3s\n",
      "[CV] C=0.1, max_iter=1000, penalty=l2, solver=lbfgs ..................\n",
      "[CV]  C=0.1, max_iter=1000, penalty=l2, solver=lbfgs, score=0.7247697236684021, total=   0.3s\n",
      "[CV] C=0.1, max_iter=1000, penalty=l2, solver=lbfgs ..................\n",
      "[CV]  C=0.1, max_iter=1000, penalty=l2, solver=lbfgs, score=0.7286744092911493, total=   0.3s\n",
      "[CV] C=0.1, max_iter=1000, penalty=l2, solver=lbfgs ..................\n",
      "[CV]  C=0.1, max_iter=1000, penalty=l2, solver=lbfgs, score=0.7190347451687193, total=   0.3s\n",
      "[CV] C=0.1, max_iter=1000, penalty=l2, solver=newton-cg ..............\n",
      "[CV]  C=0.1, max_iter=1000, penalty=l2, solver=newton-cg, score=0.7247697236684021, total=   0.5s\n",
      "[CV] C=0.1, max_iter=1000, penalty=l2, solver=newton-cg ..............\n",
      "[CV]  C=0.1, max_iter=1000, penalty=l2, solver=newton-cg, score=0.7286744092911493, total=   0.5s\n",
      "[CV] C=0.1, max_iter=1000, penalty=l2, solver=newton-cg ..............\n",
      "[CV]  C=0.1, max_iter=1000, penalty=l2, solver=sag, score=0.723568281938326, total=   5.0s\n",
      "[CV] C=0.1, max_iter=5000, penalty=l2, solver=sag ....................\n",
      "[CV]  C=0.1, max_iter=1000, penalty=l2, solver=newton-cg, score=0.7190347451687193, total=   0.6s\n",
      "[CV] C=0.1, max_iter=5000, penalty=l2, solver=sag ....................\n",
      "[CV]  C=0.1, max_iter=1000, penalty=l2, solver=sag, score=0.7285742891469764, total=   5.9s\n",
      "[CV] C=0.1, max_iter=5000, penalty=l2, solver=sag ....................\n",
      "[CV]  C=0.1, max_iter=1000, penalty=l2, solver=sag, score=0.7188344848302793, total=   5.8s\n",
      "[CV] C=0.1, max_iter=5000, penalty=l2, solver=lbfgs ..................\n",
      "[CV]  C=0.1, max_iter=5000, penalty=l2, solver=lbfgs, score=0.7247697236684021, total=   0.3s\n",
      "[CV] C=0.1, max_iter=5000, penalty=l2, solver=lbfgs ..................\n",
      "[CV]  C=0.1, max_iter=5000, penalty=l2, solver=lbfgs, score=0.7286744092911493, total=   0.3s\n",
      "[CV] C=0.1, max_iter=5000, penalty=l2, solver=lbfgs ..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  29 tasks      | elapsed:   13.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.1, max_iter=5000, penalty=l2, solver=lbfgs, score=0.7190347451687193, total=   0.3s\n",
      "[CV] C=0.1, max_iter=5000, penalty=l2, solver=newton-cg ..............\n",
      "[CV]  C=0.1, max_iter=5000, penalty=l2, solver=newton-cg, score=0.7247697236684021, total=   0.6s\n",
      "[CV] C=0.1, max_iter=5000, penalty=l2, solver=newton-cg ..............\n",
      "[CV]  C=0.1, max_iter=5000, penalty=l2, solver=newton-cg, score=0.7286744092911493, total=   0.6s\n",
      "[CV] C=0.1, max_iter=5000, penalty=l2, solver=newton-cg ..............\n",
      "[CV]  C=0.1, max_iter=5000, penalty=l2, solver=newton-cg, score=0.7190347451687193, total=   0.5s\n",
      "[CV] C=1.0, max_iter=100, penalty=l2, solver=sag .....................\n",
      "[CV]  C=0.1, max_iter=5000, penalty=l2, solver=sag, score=0.723568281938326, total=   6.0s\n",
      "[CV] C=1.0, max_iter=100, penalty=l2, solver=sag .....................\n",
      "[CV]  C=0.1, max_iter=5000, penalty=l2, solver=sag, score=0.7285742891469764, total=   6.0s\n",
      "[CV] C=1.0, max_iter=100, penalty=l2, solver=sag .....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1.0, max_iter=100, penalty=l2, solver=sag, score=0.7149579495394474, total=   1.1s\n",
      "[CV] C=1.0, max_iter=100, penalty=l2, solver=lbfgs ...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1.0, max_iter=100, penalty=l2, solver=sag, score=0.7195634761714057, total=   1.1s\n",
      "[CV] C=1.0, max_iter=100, penalty=l2, solver=lbfgs ...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1.0, max_iter=100, penalty=l2, solver=sag, score=0.707019124862321, total=   1.1s\n",
      "[CV] C=1.0, max_iter=100, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=1.0, max_iter=100, penalty=l2, solver=lbfgs, score=0.7611133360032039, total=   0.4s\n",
      "[CV] C=1.0, max_iter=100, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=1.0, max_iter=100, penalty=l2, solver=lbfgs, score=0.7604124949939928, total=   0.5s\n",
      "[CV] C=1.0, max_iter=100, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=1.0, max_iter=100, penalty=l2, solver=lbfgs, score=0.7521778311805347, total=   0.5s\n",
      "[CV] C=1.0, max_iter=100, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=1.0, max_iter=100, penalty=l2, solver=newton-cg, score=0.7611133360032039, total=   0.9s\n",
      "[CV] C=1.0, max_iter=500, penalty=l2, solver=sag .....................\n",
      "[CV]  C=1.0, max_iter=100, penalty=l2, solver=newton-cg, score=0.7601121345614738, total=   0.9s\n",
      "[CV] C=1.0, max_iter=500, penalty=l2, solver=sag .....................\n",
      "[CV]  C=1.0, max_iter=100, penalty=l2, solver=newton-cg, score=0.7519775708420947, total=   1.0s\n",
      "[CV] C=1.0, max_iter=500, penalty=l2, solver=sag .....................\n",
      "[CV]  C=0.1, max_iter=5000, penalty=l2, solver=sag, score=0.7188344848302793, total=   6.7s\n",
      "[CV] C=1.0, max_iter=500, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=1.0, max_iter=500, penalty=l2, solver=lbfgs, score=0.7611133360032039, total=   0.8s\n",
      "[CV] C=1.0, max_iter=500, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=1.0, max_iter=500, penalty=l2, solver=lbfgs, score=0.7601121345614738, total=   0.7s\n",
      "[CV] C=1.0, max_iter=500, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=1.0, max_iter=500, penalty=l2, solver=lbfgs, score=0.7519775708420947, total=   0.7s\n",
      "[CV] C=1.0, max_iter=500, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=1.0, max_iter=500, penalty=l2, solver=newton-cg, score=0.7611133360032039, total=   0.7s\n",
      "[CV] C=1.0, max_iter=500, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=1.0, max_iter=500, penalty=l2, solver=newton-cg, score=0.7601121345614738, total=   0.7s\n",
      "[CV] C=1.0, max_iter=500, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=1.0, max_iter=500, penalty=l2, solver=newton-cg, score=0.7519775708420947, total=   0.9s\n",
      "[CV] C=1.0, max_iter=1000, penalty=l2, solver=sag ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1.0, max_iter=500, penalty=l2, solver=sag, score=0.7437925510612735, total=   6.4s\n",
      "[CV] C=1.0, max_iter=1000, penalty=l2, solver=sag ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1.0, max_iter=500, penalty=l2, solver=sag, score=0.7424909891870244, total=   6.4s\n",
      "[CV] C=1.0, max_iter=1000, penalty=l2, solver=sag ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1.0, max_iter=500, penalty=l2, solver=sag, score=0.735355962751577, total=   6.2s\n",
      "[CV] C=1.0, max_iter=1000, penalty=l2, solver=lbfgs ..................\n",
      "[CV]  C=1.0, max_iter=1000, penalty=l2, solver=lbfgs, score=0.7611133360032039, total=   0.8s\n",
      "[CV] C=1.0, max_iter=1000, penalty=l2, solver=lbfgs ..................\n",
      "[CV]  C=1.0, max_iter=1000, penalty=l2, solver=lbfgs, score=0.7601121345614738, total=   0.7s\n",
      "[CV] C=1.0, max_iter=1000, penalty=l2, solver=lbfgs ..................\n",
      "[CV]  C=1.0, max_iter=1000, penalty=l2, solver=lbfgs, score=0.7519775708420947, total=   0.7s\n",
      "[CV] C=1.0, max_iter=1000, penalty=l2, solver=newton-cg ..............\n",
      "[CV]  C=1.0, max_iter=1000, penalty=l2, solver=newton-cg, score=0.7611133360032039, total=   0.8s\n",
      "[CV] C=1.0, max_iter=1000, penalty=l2, solver=newton-cg ..............\n",
      "[CV]  C=1.0, max_iter=1000, penalty=l2, solver=newton-cg, score=0.7601121345614738, total=   0.7s\n",
      "[CV] C=1.0, max_iter=1000, penalty=l2, solver=newton-cg ..............\n",
      "[CV]  C=1.0, max_iter=1000, penalty=l2, solver=newton-cg, score=0.7519775708420947, total=   0.9s\n",
      "[CV] C=1.0, max_iter=5000, penalty=l2, solver=sag ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1.0, max_iter=1000, penalty=l2, solver=sag, score=0.7514016820184221, total=  10.7s\n",
      "[CV] C=1.0, max_iter=5000, penalty=l2, solver=sag ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1.0, max_iter=1000, penalty=l2, solver=sag, score=0.742765595273856, total=  10.5s\n",
      "[CV] C=1.0, max_iter=5000, penalty=l2, solver=sag ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1.0, max_iter=1000, penalty=l2, solver=sag, score=0.7550060072086504, total=  10.6s\n",
      "[CV] C=1.0, max_iter=5000, penalty=l2, solver=lbfgs ..................\n",
      "[CV]  C=1.0, max_iter=5000, penalty=l2, solver=lbfgs, score=0.7611133360032039, total=   0.7s\n",
      "[CV] C=1.0, max_iter=5000, penalty=l2, solver=lbfgs ..................\n",
      "[CV]  C=1.0, max_iter=5000, penalty=l2, solver=lbfgs, score=0.7601121345614738, total=   0.7s\n",
      "[CV] C=1.0, max_iter=5000, penalty=l2, solver=lbfgs ..................\n",
      "[CV]  C=1.0, max_iter=5000, penalty=l2, solver=lbfgs, score=0.7519775708420947, total=   0.7s\n",
      "[CV] C=1.0, max_iter=5000, penalty=l2, solver=newton-cg ..............\n",
      "[CV]  C=1.0, max_iter=5000, penalty=l2, solver=newton-cg, score=0.7611133360032039, total=   0.8s\n",
      "[CV] C=1.0, max_iter=5000, penalty=l2, solver=newton-cg ..............\n",
      "[CV]  C=1.0, max_iter=5000, penalty=l2, solver=newton-cg, score=0.7601121345614738, total=   0.7s\n",
      "[CV] C=1.0, max_iter=5000, penalty=l2, solver=newton-cg ..............\n",
      "[CV]  C=1.0, max_iter=5000, penalty=l2, solver=newton-cg, score=0.7519775708420947, total=   0.9s\n",
      "[CV] C=10, max_iter=100, penalty=l2, solver=sag ......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, max_iter=100, penalty=l2, solver=sag, score=0.7144573488185822, total=   1.0s\n",
      "[CV] C=10, max_iter=100, penalty=l2, solver=sag ......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, max_iter=100, penalty=l2, solver=sag, score=0.7203644373247897, total=   1.0s\n",
      "[CV] C=10, max_iter=100, penalty=l2, solver=sag ......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, max_iter=100, penalty=l2, solver=sag, score=0.706518474016221, total=   1.0s\n",
      "[CV] C=10, max_iter=100, penalty=l2, solver=lbfgs ....................\n",
      "[CV]  C=10, max_iter=100, penalty=l2, solver=lbfgs, score=0.775330396475771, total=   0.4s\n",
      "[CV] C=10, max_iter=100, penalty=l2, solver=lbfgs ....................\n",
      "[CV]  C=10, max_iter=100, penalty=l2, solver=lbfgs, score=0.7789347216659992, total=   0.4s\n",
      "[CV] C=10, max_iter=100, penalty=l2, solver=lbfgs ....................\n",
      "[CV]  C=10, max_iter=100, penalty=l2, solver=lbfgs, score=0.7698007409632522, total=   0.4s\n",
      "[CV] C=10, max_iter=100, penalty=l2, solver=newton-cg ................\n",
      "[CV]  C=10, max_iter=100, penalty=l2, solver=newton-cg, score=0.776531838205847, total=   1.9s\n",
      "[CV] C=10, max_iter=100, penalty=l2, solver=newton-cg ................\n",
      "[CV]  C=10, max_iter=100, penalty=l2, solver=newton-cg, score=0.7829395274329195, total=   1.7s\n",
      "[CV] C=10, max_iter=100, penalty=l2, solver=newton-cg ................\n",
      "[CV]  C=10, max_iter=100, penalty=l2, solver=newton-cg, score=0.7725042555321918, total=   1.9s\n",
      "[CV] C=10, max_iter=500, penalty=l2, solver=sag ......................\n",
      "[CV]  C=1.0, max_iter=5000, penalty=l2, solver=sag, score=0.7595114136964357, total=  23.1s\n",
      "[CV] C=10, max_iter=500, penalty=l2, solver=sag ......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, max_iter=500, penalty=l2, solver=sag, score=0.7441930316379656, total=   4.7s\n",
      "[CV] C=10, max_iter=500, penalty=l2, solver=sag ......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, max_iter=500, penalty=l2, solver=sag, score=0.7464957949539447, total=   4.7s\n",
      "[CV] C=10, max_iter=500, penalty=l2, solver=lbfgs ....................\n",
      "[CV]  C=1.0, max_iter=5000, penalty=l2, solver=sag, score=0.749474316611595, total=  22.9s\n",
      "[CV] C=10, max_iter=500, penalty=l2, solver=lbfgs ....................\n",
      "[CV]  C=1.0, max_iter=5000, penalty=l2, solver=sag, score=0.7595114136964357, total=  24.0s\n",
      "[CV] C=10, max_iter=500, penalty=l2, solver=lbfgs ....................\n",
      "[CV]  C=10, max_iter=500, penalty=l2, solver=lbfgs, score=0.776531838205847, total=   1.6s\n",
      "[CV] C=10, max_iter=500, penalty=l2, solver=newton-cg ................\n",
      "[CV]  C=10, max_iter=500, penalty=l2, solver=lbfgs, score=0.7826391670004005, total=   1.9s\n",
      "[CV] C=10, max_iter=500, penalty=l2, solver=newton-cg ................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, max_iter=500, penalty=l2, solver=sag, score=0.7369580454590968, total=   5.7s\n",
      "[CV] C=10, max_iter=500, penalty=l2, solver=newton-cg ................\n",
      "[CV]  C=10, max_iter=500, penalty=l2, solver=lbfgs, score=0.7725042555321918, total=   2.0s\n",
      "[CV] C=10, max_iter=1000, penalty=l2, solver=sag .....................\n",
      "[CV]  C=10, max_iter=500, penalty=l2, solver=newton-cg, score=0.776531838205847, total=   2.3s\n",
      "[CV] C=10, max_iter=1000, penalty=l2, solver=sag .....................\n",
      "[CV]  C=10, max_iter=500, penalty=l2, solver=newton-cg, score=0.7829395274329195, total=   2.1s\n",
      "[CV] C=10, max_iter=1000, penalty=l2, solver=sag .....................\n",
      "[CV]  C=10, max_iter=500, penalty=l2, solver=newton-cg, score=0.7725042555321918, total=   2.4s\n",
      "[CV] C=10, max_iter=1000, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=10, max_iter=1000, penalty=l2, solver=lbfgs, score=0.776531838205847, total=   1.7s\n",
      "[CV] C=10, max_iter=1000, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=10, max_iter=1000, penalty=l2, solver=lbfgs, score=0.7826391670004005, total=   1.7s\n",
      "[CV] C=10, max_iter=1000, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=10, max_iter=1000, penalty=l2, solver=lbfgs, score=0.7725042555321918, total=   1.5s\n",
      "[CV] C=10, max_iter=1000, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=10, max_iter=1000, penalty=l2, solver=newton-cg, score=0.776531838205847, total=   1.8s\n",
      "[CV] C=10, max_iter=1000, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=10, max_iter=1000, penalty=l2, solver=newton-cg, score=0.7829395274329195, total=   1.7s\n",
      "[CV] C=10, max_iter=1000, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=10, max_iter=1000, penalty=l2, solver=newton-cg, score=0.7725042555321918, total=   1.8s\n",
      "[CV] C=10, max_iter=5000, penalty=l2, solver=sag .....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, max_iter=1000, penalty=l2, solver=sag, score=0.7559070885062075, total=  13.4s\n",
      "[CV] C=10, max_iter=5000, penalty=l2, solver=sag .....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, max_iter=1000, penalty=l2, solver=sag, score=0.7558069683620344, total=  13.1s\n",
      "[CV] C=10, max_iter=5000, penalty=l2, solver=sag .....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikgreenj/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, max_iter=1000, penalty=l2, solver=sag, score=0.7459697606888955, total=  12.8s\n",
      "[CV] C=10, max_iter=5000, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=10, max_iter=5000, penalty=l2, solver=lbfgs, score=0.776531838205847, total=   1.5s\n",
      "[CV] C=10, max_iter=5000, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=10, max_iter=5000, penalty=l2, solver=lbfgs, score=0.7826391670004005, total=   1.6s\n",
      "[CV] C=10, max_iter=5000, penalty=l2, solver=lbfgs ...................\n",
      "[CV]  C=10, max_iter=5000, penalty=l2, solver=lbfgs, score=0.7725042555321918, total=   1.6s\n",
      "[CV] C=10, max_iter=5000, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=10, max_iter=5000, penalty=l2, solver=newton-cg, score=0.776531838205847, total=   1.8s\n",
      "[CV] C=10, max_iter=5000, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=10, max_iter=5000, penalty=l2, solver=newton-cg, score=0.7829395274329195, total=   1.6s\n",
      "[CV] C=10, max_iter=5000, penalty=l2, solver=newton-cg ...............\n",
      "[CV]  C=10, max_iter=5000, penalty=l2, solver=newton-cg, score=0.7725042555321918, total=   1.8s\n",
      "[CV]  C=10, max_iter=5000, penalty=l2, solver=sag, score=0.7608891559026735, total=  36.7s\n",
      "[CV]  C=10, max_iter=5000, penalty=l2, solver=sag, score=0.7749299158990789, total=  37.6s\n",
      "[CV]  C=10, max_iter=5000, penalty=l2, solver=sag, score=0.7714257108530236, total=  40.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 108 out of 108 | elapsed:  1.9min finished\n"
     ]
    }
   ],
   "source": [
    "grid_params = {\n",
    "    'solver' : ['sag', 'lbfgs', 'newton-cg'],\n",
    "    'penalty': ['l2'],\n",
    "    'C' : [.1, 1.0, 10],\n",
    "    'max_iter' : [100, 500, 1000, 5000]\n",
    "}\n",
    "\n",
    "gs_log = GridSearchCV(\n",
    "    LogisticRegression(),\n",
    "    grid_params,\n",
    "    verbose = 2.5,\n",
    "    n_jobs = 4\n",
    ")\n",
    "\n",
    "gs_log_results = gs_log.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'C': 10, 'max_iter': 100, 'penalty': 'l2', 'solver': 'newton-cg'},\n",
       " 0.7773253679538097)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_log.best_params_, gs_log.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8652337883389514"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_log.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=10000, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='newton-cg', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logr = LogisticRegression(C=100, solver='newton-cg', max_iter=10000)\n",
    "logr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8824550278677035"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logr.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7965589320217812"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With hyperparameter tuning, got better scores, but evidence of overfitting exists.  \n",
    "##### Given more time to experiment, I would feed Logreg higher %s of the dataset and tune a bit more.\n",
    "##### As is, still pleased enough with the results of RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = svm.SVC(C=.1, kernel='linear')\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8080632780429196"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7528297909713684"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVC scores not great, but could have promise?  Considering a GridSearchCV to try other kernel tricks but this process is computationally expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV] C=0.1, kernel=linear ............................................\n",
      "[CV] C=0.1, kernel=linear ............................................\n",
      "[CV] C=0.1, kernel=linear ............................................\n",
      "[CV] C=0.1, kernel=rbf ...............................................\n",
      "[CV] C=0.1, kernel=rbf ...............................................\n",
      "[CV] C=0.1, kernel=rbf ...............................................\n",
      "[CV] C=0.1, kernel=sigmoid ...........................................\n",
      "[CV] C=0.1, kernel=sigmoid ...........................................\n",
      "[CV] ... C=0.1, kernel=linear, score=0.7358830596716059, total= 1.4min\n",
      "[CV] C=0.1, kernel=sigmoid ...........................................\n",
      "[CV] ... C=0.1, kernel=linear, score=0.7303494542905777, total= 1.6min\n",
      "[CV] C=0.1, kernel=precomputed .......................................\n",
      "[CV] C=0.1, kernel=precomputed .......................................\n",
      "[CV] C=0.1, kernel=precomputed .......................................\n",
      "[CV] C=1.0, kernel=linear ............................................\n",
      "[CV] ... C=0.1, kernel=linear, score=0.7343812575090108, total= 1.6min\n",
      "[CV] C=1.0, kernel=linear ............................................\n",
      "[CV] ... C=0.1, kernel=sigmoid, score=0.500100120144173, total= 1.8min\n",
      "[CV] C=1.0, kernel=linear ............................................\n",
      "[CV] ... C=0.1, kernel=sigmoid, score=0.500100120144173, total= 1.8min\n",
      "[CV] C=1.0, kernel=rbf ...............................................\n",
      "[CV] ...... C=0.1, kernel=rbf, score=0.5020024028834601, total= 2.0min\n",
      "[CV] C=1.0, kernel=rbf ...............................................\n",
      "[CV] ...... C=0.1, kernel=rbf, score=0.5220787023130069, total= 2.0min\n",
      "[CV] C=1.0, kernel=rbf ...............................................\n",
      "[CV] ...... C=0.1, kernel=rbf, score=0.5085102122547056, total= 2.0min\n",
      "[CV] C=1.0, kernel=sigmoid ...........................................\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-55af6b08ab94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mgs_SVC_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs_SVC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid_params = {\n",
    "    'kernel' : ['linear', 'rbf', 'sigmoid', 'precomputed'],  # all kernels except poly\n",
    "    'C' : [.1, 1.0, 10],\n",
    "}\n",
    "\n",
    "gs_SVC = GridSearchCV(\n",
    "    svm.SVC(),\n",
    "    grid_params,\n",
    "    verbose = 2.5,\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "gs_SVC_results = gs_SVC.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminated GridSearchCV as no good results were coming out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion for Model selection, RandomForestClassifier was the best model of these 4 without a doubt.  I wouldn't consider putting these into a Voting Classifier, because it would likely return worse results than the RFC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting total word counts for each class (top 100 words) to put in word clouds for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMvectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   #stopwords already removed\n",
    "                             max_features = 300,\n",
    "                             max_df=1.0, #default\n",
    "                             min_df=1    #default\n",
    "                            )\n",
    "DEMvect = DEMvectorizer.fit_transform(df[df['subreddit'] == 1].body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMwords = pd.DataFrame(DEMvect.todense(), columns=DEMvectorizer.get_feature_names(), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "counts = []\n",
    "for x in list(DEMwords.columns):\n",
    "    #if sum(DEMwords[f'{x}'])//3871 < 8:\n",
    "    words.append(x) \n",
    "    counts.append(sum(DEMwords[f'{x}'])//3871)       #floor dividing by the lowest count, to maintain proportion\n",
    "\n",
    "count_dict = dict(zip(words, counts))\n",
    "count_frame = pd.DataFrame.from_dict(count_dict, orient='index')\n",
    "count_frame.sort_values(by=0, ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0\n",
       "people  15\n",
       "trump   14"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dividing all totals by lowest count to maintain proportion but allow it to fit in the cloud.\n",
    "#min(counts)\n",
    "count_frame.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a proportional list, by iterating through count_dict made above\n",
    "\n",
    "dem_cloud_list = []\n",
    "for k,v in count_dict.items():\n",
    "    for x in range(v):                      #We will append word(k) v times\n",
    "        dem_cloud_list.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "626"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dem_cloud_list)      #this returns a proportional list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dem_cloud_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPvectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   #stopwords already removed\n",
    "                             max_features = 300,\n",
    "                             max_df=1.0, #default\n",
    "                             min_df=1    #default\n",
    "                            )\n",
    "REPvect = REPvectorizer.fit_transform(df[df['subreddit'] == 0].body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPwords = pd.DataFrame(REPvect.todense(), columns=REPvectorizer.get_feature_names(), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rwords = []\n",
    "Rcounts = []\n",
    "for x in list(REPwords.columns):\n",
    "    #if sum(REPwords[f'{x}'])//4107 < 7:\n",
    "    Rwords.append(x) \n",
    "    Rcounts.append(sum(REPwords[f'{x}'])//4107)\n",
    "\n",
    "Rcount_dict = dict(zip(Rwords, Rcounts))\n",
    "Rcount_frame = pd.DataFrame.from_dict(Rcount_dict, orient='index')\n",
    "Rcount_frame.sort_values(by=0, ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>republican</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "trump       17\n",
       "republican  13"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rcount_frame.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_cloud_list = []\n",
    "for k,v in Rcount_dict.items():\n",
    "    for x in range(v):                      #We will append word(k) v times\n",
    "        rep_cloud_list.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "645"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rep_cloud_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rep_cloud_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the proportional clouds in wordclouds.com and took screenshot for presentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
